
Moving Beyond Apple's iOS to the Future — (noctivagous)


noctivagous.github.io


The iOS UI Is Reductive; It Limits Tablet Computers

For tablet computers, iOS (iPadOS) carries a major drawback: the simplest tasks are easy to achieve, but anything slightly complicated has to be specifically learned because it is invisible, offscreen, or buried in conventions.   The "user friendly experience" only describes the basics of launching an app, zooming inside a document, and scrolling.  After that, the system software interface features are tacked on, one after another.  The consequence is that a user can easily activate one of iOS's side features on accident, and this is a poor state of UI design.  But also, iOS is an undeveloped and prototype-level collection of user interface conventions in the first place, incomplete except for consumption of media.  Because it strives towards minimalism and commercial accessibility, it is nubby and dumbed down, limited for professional usage.  For the user's control of the tablet software, Apple, characteristically, insists on withholding from the user any complementary physical controls such as knobs, sliders, and buttons, believing that those were the problem in the first place.

If there are so many problems with the tablet UI, where does Apple's success come from?  Steve Jobs just knew what mattered at the time, to put together a tablet UI that worked for the hands and didn't require a stylus to navigate the interface. To accomplish that, he brought in the multitouch technologies that had been recently upgraded by researchers (see 2006 video: https://www.youtube.com/watch?v=ac0E6deG4AU).  This technology had been available for a few decades but had been unutilized for consumer products.   The minimalist design ideology Jobs brought to the device was a secondary aspect, and it isn't the actual foundation of the iPad's success.  Rather, iOS is just a minimalist's implementation of multitouch screen and "gestures" on a tablet, artificially restricting it to sealed off "apps," which is a self-serving format for tablet software.

In chronological order, Apple developed first a large, tablet-sized multitouch screen and then shrank that to make the iPhone. Then a few years later Apple released a multitouch interface device in its original, larger size as the iPad.  It is of course very popular and has also become a definitive reference for other companies that no one bothers to re-examine, unfortunately.  Embedding a foolproof, kiosk type of user interface inside a mobile tablet can produce a lot of sales because anyone from the public can use a kiosk and thus anyone can use what iOS provides.  But it won't ever reach the depth of a PC's (a desktop computer's) capabilities.  That is the current problem, that no company has manufactured a truly finished touchscreen tablet UI, as Android is hardly more than a clone of Apple's products.  Application software will never be programmed on the iPad itself without the use of desktop computer accessories that make it behave like a PC.  Currently Apple's software development system (Xcode) is not available for iPads.  The stock UI of an iPad cannot accommodate it.

It's the design ideology behind the user interface that has caused this. Making a device blunt in its simplicity comes at a steep cost because eventually the apps that people want to make and use become complex, and software developers lack a systematic tablet UI scheme fulfilling their needs that could mirror desktop computer functionality.  When iOS is ideologically narrow, deeply determined to package bare interactive essentials as the environment for mobile life, it doesn’t possess any conceptual frameworks to accommodate complex usage of a tablet computer.  So necessarily there are PC trackpads and keyboard sold for these tablets, to turn them into PCs with screen cursors.   No screen cursor would be necessary if iOS carried a complete set of UI principles, but achieving that will involve breaking out of Apple's own ideology that shuns accessory physical controls on the mobile device.  Whenever the tablet software is intended for serious usage, a trackpad basically has to be utilized.  Sony, by contrast, did not take this viewpoint about external controls and it is Apple's that has spread across society instead.

The theory of the PC was developed over a period of years by Douglas Engelbart, and later Xerox PARC, so a tablet's return to the desktop computer's conventions is inevitable when nothing comparable in depth has been developed for a multitouch device.  Apple and even Steve Jobs did not grasp this, as they were sure that the iPad could result in the "post-PC era" as soon as it was launched, indicating that Apple has always been terribly superficial when it comes to theories that produce the real computer experience.  The theory behind iOS isn't much beyond a set of raw conveniences to accommodate the multitouch “gestures” unveiled by researchers in the 2000s, serving to launch apps and press buttons inside iOS apps.  Apple's specific packaging of that effectively halted the industry's future development of touchscreen UI because manufacturers were only willing to copy Apple and Apple itself was holding to its own ideology that doesn't go very far.  For example, iOS is supposedly intuitive but it's difficult for the user to figure out how to open a file inside a specific application or how to manage the filesystem in a sophisticated way because the files are no longer the focus, just launching kiosk-like apps, for commercialized simplicity's sake.  The objective of making everything user friendly the point of being foolproof has resulted in an uncomfortable set of devices for several years now that can scroll, zoom, activate buttons but which severely limit engagement with the mobile computer's wide-ranging capabilities.

The evolution of the touchscreen tablet UI can begin by discussing what kinds of controls can be added to the device and what purpose they will serve.



Touch Operation Buttons

An object sits there on the tablet touchscreen, but what does touching it do?  In real life it reacts according to physics.  On a touchscreen computer, the initial answer is vague, but generally "open" or "activate".  This is an operation, of course, and that is what differentiates the real world physics from the conventions of a touch event on a tablet device.   On iOS, a single touch is basically defined as one operation: "activate".  To make the touch do anything else, beyond that function requires learning all of the complicated conventions afterwards, such as press down for longer to get something to appear.  The touch could be made to do lots of different functions, but right now it can only do "activate".

If several physical buttons were arranged in a column just to the left of the screen, they could to specify what a touch means when it occurs, and this will broaden the tablet's user interface a great deal and make the UI far more nimble.  Closing a window would not involve touching a close "x" that appears on an object but instead momentarily holding down the "Close" touch operation button on the left edge of the screen and then touching any part of window(s), panel(s), or object(s) that need to be closed.    When a certain touch operation button is held down, e.g. an "inspect" button, whatever object the finger touches will be acted upon accordingly (it will be inspected, it will be closed, etc.).  More than one object can receive that operation while the button is held down, making for a quick and capable UI.

To summarize: a touch operation button momentarily specifies what a touch/gesture means just before the touch occurs and then the physical button is released by the user.  The speed and straightforwardness of the interface improves: five objects can be removed from a page quickly by holding down a "remove" operation button, then tapping each of the five objects, then releasing the "remove" touch button that sits to the left of the screen.   This setup will allow the user to control software with a wide range of touch operations instead of searching for operations deep inside the interface through various convoluted maneuvers.  In addition, the physical touch operation buttons themselves can have their operations swapped out when they each carry individual LCD screens.  

Thus what is convoluted to achieve today on a tablet will not be, and what is out of reach for the average user who does not bother to memorize specific features will become straightforward for anyone when these physical buttons are placed onto the tablet device.  The key benefit when there are several touch operation buttons sitting to the left of the screen is that they can accommodate a wide range of touch objectives immediately, and this is a core of a new tablet device framework for user interaction.  A “generate” touch operation button will make the touch generate an object at the position of contact in an empty space (or it will generate a child object for the object that was touched).  To illustrate: five objects can be generated by holding down the "generate" touch operation button and then tapping the desired locations of the five objects.  For power users, some of these touch operation buttons can designated for custom-programmed operations and scripts.  Each touch operation button has an LCD screen.

Importantly, in this scheme the user will always know how to accomplish tasks because the touch operation buttons naturally work in combination with each other, being general in their function names, covering a broad range of activities. "Close" can apply to exiting or closing anything, usable for many types of objects on screen.  "Inspect" can apply to a variety of objects as well.   Just these two example buttons already work in combination: any inspection popover opened with "Inspect" can be closed with the touch operation button "Close", demonstrating that the operations work systematically already.  A user can know how to do all sorts of tasks without specific instructions when the provided operation buttons are helpful.  The current procedure for the user in iOS is to extract these functions (making close buttons appear or digging into the user interface to find the desired function) to activate them with touch) and this is no longer necessary.

More touch operation buttons:

-- "Teleport": instead of dragging, the first touch establishes what object is to be transported to another part of the screen (what would be dragged) and the second touch moves it to its desired location immediately, thus the name teleportation.  So there are only two touches to accomplish what dragging does today.  This is much faster than dragging objects on a touchscreen and will be much more precise.  To recap, the "Teleport" touch operation is held down during the first and second touch, then it is released.  This opens the door to other types of multiple taps: if it were "Copy" that were held down, the first touch would establish what is to be copied and the other touches would be copy destinations.  Then after all of the touches, "Copy" is released.

-- "Toggle" will turn a touched setting or object on and off, opposite its current state and this can apply to many circumstances.

Fortunately, the touch operation buttons reduce the need for what are currently called "gestures" because even a "Zoom" touch operation button will provide more precise control over the zoom as "Zoom" is being held down, allowing the user to go even slower than today.  Likewise, a "Rotate" touch operation button will provide greater real-time precision for the user than a rotate "gesture". In the case it is still desired to have "gestures," there can be many types of swipe, rotate, or two-finger scrolling according to the touch operation buttons. Since zoom, rotate, and pan are such commonly accessed features they may have their own dedicated buttons on the device anyway.   If they do not the remaining advantage of "gestures" is that they do not occupy space in the touch operations columns.

Touch operation buttons should serve to replace digging into menus to find the desired function, which is the current dynamic.  If the labels for these touch operation buttons are categorized and grouped adequately, an increase in the number of operations will not overwhelm the user but instead make it so the user does not need to be provided a tutorial for every task.  In this way, touch operation buttons can be categorized and assigned corresponding background colors on the LCD-backed buttons, allowing the user to know what a touch operation button is for.  The question of how to access more than one group of touch operation buttons on the column of buttons becomes an immediate questions.  Forward and backward arrows are probably not sufficient.  Instead, there is a separate touch operation group grid beneath the column, and each class of touch operation is a button. A class of touch operation buttons can be accessed according to the color and label on this grid of touch operation groups buttons. 



Opposing Touch Operation Buttons

In this column of physical touch operation buttons on the left side of the scren, it might seem best at first to have just one column.   But often operations exist in opposition to each other (group --- ungroup,  add --- subtract,  generate --- remove, open --- close, show --- hide).  Two columns of opposing operations might be preferable, doubling the number of available buttons sitting to the left of the screen.

A window is closed with "Close," but "Close" can also be used to turn a switch control to the off position and close just about anything.  "Open" opens anything in the same way and turns something on.  Pairs of buttons would have generic names: such as "Open" paired with "Close", which apply to many scenarios of all sorts.

"Expand  --- Collapse" is about unfolding and stowing away and applies to windows but also collapsible controls.  That pair of operations addresses a different state dynamic than "Open --- Close".



The "Multiple" Touch Operation Modifier

Like the shift key on the keyboard, the "Multiple" key on the device allows selecting more than one object at a time.  It is a modifier button, which is what the shift key is.  During typing it acts as the shift key, too.  Where this key is placed is not certain at the moment, but could be on the side, where less frequently accessed keys are normally accessed.



Mode Buttons Column

On the right side of the device's screen can sit mode buttons that are utilized by all apps.  These buttons do not modify the touch but instead the entirety of the app's mode or the mode of a certain context.  All apps running on the tablet would utilize these buttons as a primary means of interaction with the app, because all complex apps have states or modes.   Again, each of these buttons will be backed by individual LCD screens to give the software developer's flexibility.  The purpose of these mode buttons is to support anything that is a mode or state.  Most apps have various modes: viewing and editing being the most fundamental for apps that produce data of some sort.  Configuring preferences is another mode and when both the software developer and the user thinks in terms of "mode," because the device is built that way, there is no need for the user to look up specific conventions for opening the settings window.  (It's also possible to make a "Configure" touch operation button so that with a touch anything on the screen can be configured).

Other uses of modes include switching on and off special submodes of editing in the app.  The mode buttons should be able to accommodate anything needed involving modes, submodes, and states. 

The conceptual internals of an app and computing system are being represented on the outside of this device, around the screen.



Thumb Input for Text Words and Characters: Mode Buttons and Touch Operation Buttons Simultaneously Enter a Meta Mode, to Replace The Conventional Onscreen Keyboard, Providing an Input Dynamic Through Alternating Thumb Input from Each Side

When typing is needed, the LCD-backed buttons on the device enter a meta mode for this purpose.  At the same time, the mode buttons on the right and the touch operation buttons on the left transform into input buttons for a specific word and typing entry system, with each of the two sides able to receive input back and forth in alternation by the thumbs, for the user to enter words and characters.  The two sets of buttons have entered a meta mode for input, repurposed for entering words. What shows up on the LCD screens for each button on both sides can go beyond individual characters of a keyboard, to allow entering words without autocomplete. The entry will be both tactile and accurate because the buttons themselves are physical and it can be the alternation of pressing buttons on both sides that completes words.  The buttons can display anything, including full words and even symbols or images, so meta modes are an expansive area in the case there are other needs for all of the buttons.  What is static on the device are reaction buttons such as "Escape/Cancel" in the upper lefthand corner" and "Accept/Proceed" in the lower right corner.

What is pressed on the buttons can affect what shows up on the left and right sides of the screen vertically, lining up with the buttons themselves, so that the user of the device can see the effects of pressing buttons in alternation on the screen.

A meta mode is available tool for any app for when complex entry of data would benefit from taking over the device's buttons and putting them into a special mode, but it would be utilized less commonly than what is provided.  At all times the "Escape/Cancel" button on the device would exit such a custom meta mode.

Lastly, it is possible to supplement this data entry meta mode through buttons placed on the underside of the device (as currently found on handheld video game consoles), and it is possible this would be an important part of text entry.



Reaction Buttons Such as "Undo" and "Redo"

One could say that pressing "Undo" is unlike other types of functions activated in that it is a reaction to a mistake, thus this is a reaction type of function or button.   When something relates to reacting, it is a candidate for a physical button instead of space on the touchscreen.  In an app, "Undo" and "Redo" act on a maintained list of user operation history that is most often out of view.  The two "Undo" and "Redo" buttons will not exist as touch operations because they act on the last operation done.  They sit next to the touch operation buttons because they undo or redo what was just done by a touch operation.

Reaction buttons can be said to be those which are used by the user quickly in response to an immediate change or request from the app. The periphery of the device is a suitable place to place these reaction buttons because the hands are close.  For example, if anything isn't right or should be exited, the user will want to press the "Escape" button in the top left of the device and it takes no time to press it.  If something needs to be confirmed, the user can press the "Accept/Proceed" key in the bottom right corner of the device.  Why include physical buttons when a person can touch the screen itself?  Because the less movement of the hands the better with respect to these types of user actions.



No Confirmation Dialog Boxes ("Are You Sure?") Needed Because of Implicit Confirmation Pathways

If an object is selected and it is of sensitive value for deletion (deleting it could create a problem if done on accident), then the following is how to delete it: the "Remove" touch operation button is held down and that object is tapped three times, with the object darkening with each tap.  On the third tap it is deleted. Thus, in this case there is no need to display a warning dialog box for the user that asks, "Are you sure you want to delete this?"  The pathway for the actionhas been set up so that confirmation is implicit and takes place through the action.  Many types of confirmation dialog boxes can be eliminated this way, such as through confirmation sequences where two objects are tapped in alternation or in a pattern.



Scrolling-Averse System Conventions

Noctivagous takes the position that a large amount of vertical scrolling is harmful to the user, is uncomfortable, and is a substitute for what should actually exist more often, which is proper transitioning of content (sliding, for example) that is at all times correctly sized to the device's frame dimensions.  That is, content should generally be swapped in and out carousel-style or otherwise, faded in smoothly.  It can be paginated more often and the device itself would carry two buttons for forward and backwards.  Content should be clipped less and therefore require scrolling les.  Information in the future can be prepared and laid out such that it is fitted for the dimensions of the device. "Forward" and  "Backwards" slides or fades in content within the same frame, just like any ebook reader for example, and nothing has to be moved into the frame of the device through scrolling.  This is especially applicable to consumption of information.  The web itself should be refitted in this way for long pages.



Separate X and Y Axis Touchpad Strips

A hardware feature that is considered, but is optional, for a Noctivagous tablet is a pair of X (horizontal) and Y (vertical) axis touchpad strips with pressure sensitivity.  They would sit above and to the left of the screen respectively and match the lengths of the width and height.  The Y axis (vertical) touchpad strip will sit on the left edge of the screen, sitting between the Touch Operation buttons and the screen.  These strips are available for all apps and the benefit is that they constrain movement.  Apps will be able to use these touchpad strips for horizontally and vertically constrained needs.  For irregular movements the screen itself can be used.  It is natural to select, for example, an app icon by lining up the x and y axes with both fingers as they are sliding along the touchpad and then pressing down on the two touchpad strips at the same time.





NOCTIVAGOUS.GITHUB.IO




----


With touch operations, it is also possible that something is deleted not through the use of a "delete" modifier next to a "close" modifier but instead tapping an object twice with "close," with it darkening as a warning that it will be deleted will be the way to delete something.  Other modifier keys include "route" such as for opening a file in a specific application, because "route" is a very generic keyword that can apply to many situations.


Instead of "cut [out]","copy [from]","paste [on/into]" more like "extract","


  


NOCTIVAGOUS.GITHUB.IO