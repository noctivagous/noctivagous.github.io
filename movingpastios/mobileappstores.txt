

The Issue of Converting User Interfaces from Desktop to Touchscreens


NOCTIVAGOUS.GITHUB.IO

It is often observed that the UI for iOS and Android tablets are incomplete because in the end for serious work a trackpad and keyboard have to be attached.  Related to this are the problems with the Apple Vision Pro and all VR technology.  multi-faceted, and there is no way this technology can succeed on par with desktop computers without addressing what these devices lack in terms of UI and what their teams are not doing correctly.  First, new technologies that are introduced have to be rooted first in what people need in life, not what will entertain or amuse them, because entertainment and gadgetry do not branch out to utility; it is the other way around, and utility is the core of technological progress and adoption.   Specific to VR,  however, is the matter of how both ends of the UI have to be modified for the tech to make sense: both apps and major websites have to implement support for 3D versions of what they show in 2D (such as what Magic Leap had suggested, https://github.com/magicleap/detached_explainer). Interfaces inside the VR headsets have to be essentially in 3D, not flat sheets of paper. These companies understand the UI theory halfway: in 2010, Apple executives, including Steve Jobs, asserted that the iPad would result in the "post-PC era," revealing how little Apple understood about what it was doing, how what it actually had done is incorporate the multi-touch technologies that had been around for a long time and fused them to its own operating system.  The public believes Apple is the supreme expert but actually their engineers and user interface experts work inside very narrow conventions shaped by commercial pressures.  Within those, they work extremely hard, paying close attention to every pixel, but they miss the larger picture all the time.

The first tablet computers that appeared in the 2000s presented the consumer with an unmodified desktop computer screen. Interacting with the software was tied to the use of a stylus.  Apart from omitting multi-touch functionality on the screen, the computer companies did not recognize that the input and interface to the operating system had changed, moving from mouse buttons and a screen cursor to intermittent touches that leave no lasting impression like a screen cursor.  That is, no conversion of the overall system user interface or the apps on these devices was undertaken.  As would have to be the case, these products carried little appeal except for niche users. They ended up providing, for a mobile situation, a more complicated way to use a mouse cursor and PC operating system.

For the same reason that a car’s frame and chassis cannot be directly utilized for the fuselage of an airplane or the hull of a boat, the desktop PC operating system, its cursor, and the software it runs, cannot be left the same for a touchscreen tablet or smartphone device.  This applies to all situations, including VR. When the overall input configuration and form of the hardware changes, everything else in the conventions of apps should react too.  In some cases this is apparent to everyone as part of the technology shift, such as how VR games have to be developed differently than traditional 3D video games, and so everyone makes the change.  In other subjects, it is less obvious and continues to exist in an inadequate state, such as how video game controllers still do not detect how much force has been applied to their buttons, even as players instinctively play as if they do. Each button is like an on-off light switch, including the arrow buttons, and the character on screen will jump to a fixed height, not higher if the button is pressed harder or faster.  If that one aspect of a controller changes, all of the video game companies have to change too, and the games themselves will feel more normal.

On a touchscreen, the finger is swooping in and touching the screen's graphical buttons, not a permanent cursor being pushed around, so all apps have to be written or converted for that.  Adaptation has to occur because the computer user can't easily use touch for desktop apps because the desktop apps’ buttons were made for a screen cursor, which is small, and they are smaller than what is acceptable for the fingers.  Further, when it is a touchscreen device that usually has no keyboard attached, this means that all of the keyboard (hotkey) command shortcuts cannot be activated.  Therefore, computing conventions have to match the form of the hardware, whether it is conversion of what is already used (PC desktop to iOS multi-touch) or something built from the ground up (an altogether new type of tablet PC carrying few UI connections to desktop).  iOS undertook conversion, as this was easy, but what is superior is building from the ground up, starting over for the form factor.  The failure of early tablet PCs can be explained as follows: if it is conversion of desktop that is taking place, what was originally the desktop computer interface has to be restructured for apps that sit on a different-looking machine and they end up looking different; the apps as they are developed have to be rearranged for touches, following a consistent set of rules for bigger buttons.  Under Steve Jobs, Apple went about making that kind of modification for Mac OS, transforming its exterior into a separate touch operating system, iOS, for its own type of apps.  It isn't a deep system-level change, however.  iOS is mostly a set of external software conventions to convert desktop executable conventions to touch-based apps, to make apps easy to touch and use.  They have all the essential insides of the earlier desktop apps, to the extent Jobs originally explained iOS as Mac OS on the phone.

This change of user interface has nothing to do with what software can be permitted for execution by the computer itself or the user.  It has no connection to that, whatsoever.  In addition, not everyone would implement the surrounding touch operating system conventions in such a kiosk-like, minimalistic, and restrictive way like Apple or Google chose.  With all of the differences occurring between mobile tablets and desktop computers, an opportunity arose for Apple and Google to deceive the public, that this change in computing format necessitated a restrictive environment on how apps are loaded and executed on the hardware, coinciding with a simplistic style of ease of use, that the two companies need to vet any app executed from within an online store, prohibit custom or independent app execution by the user, and take a large percentage of sales at the same time.  This part is completely artificial, a point made exceedingly obvious by Microsoft Surface tablets: these tablets run any Windows app the user wishes or downloads from anywhere, and they are touchscreen devices.  Microsoft just never cared much for app store profits and it never bothered to make a mobile, touch mode of Windows for Microsoft Surface devices that provided a framework for mobile-specific apps, so these tablets just run desktop apps and carry the problems just mentioned, that  it will be desktop software being used in an awkward way if the software engineer isn't provided a separate set of conventions for touch software (like iOS and Android).   Microsoft could have done this and provided its Surface tablets a set of Windows OS extensions for mobile, offering a kit for software developers to use, and then left access to regular Windows apps on the side for those who hook up a physical keyboard and trackpad.  In the same way, iPads could be made to run Mac OS apps on the side.  The iPad Pro even has the same processor (M2) as the latest desktop and notebook Macs.  This is why mobile devices could run touchscreen apps just freely as desktop machines run theirs.  The enormous effort to convince the public and the government otherwise is entirely motivated by the desire for profit.

What Apple and Google did was make the kiosk-like grid launch screen and the app store the center of the user experience with these devices and then confine the user to them.  An app store is just an app.  At the center of this is that profits are too high to these companies to do the right thing and open the mobile operating systems back up to normalcy, which means they can be used freely by pro users and software developers to execute anything.  It has reached the point that everyone who uses a phone is an unwitting participant of this high fee that Apple and Google imposes on the touchscreen device software developer.  In recent years, Apple has been trying to push this profit scheme onto the desktop software developers too, actively discouraging users from opening any Mac OS app that was not downloaded and vetted by the Mac OS App Store, with popup warning boxes.  All of this is an attempt to satisfy more greed and amass more control over the computer users and extract more money from the software developers selling the software.

------


People are upset that they can't run iPad apps outside of an app store.  Microsoft Surface tablets don't have this problem, but people don't want to buy them because they use touch input on desktop apps.

Previous to Steve Jobs' iPad, launched in 2010, the fundamental problem with tablet computers was that the apps and the system UI were left the same as desktop. The vital step of converting the user interface from mouse cursor to touches had not been undertaken by anyone, indicating a lack of understanding of UI principles by the industry.  Strangely, after more than a decade Microsoft still has not become aware of this and today ships its Microsoft Surface tablets computers without bundling a separate touch mode for Windows apps (which would have to be matched with a corresponding SDK for software developers so they can make touch-specific presentation).  On this matter Microsoft and it followers lament an isolated and unrelated concern, that its touch-based smartphone did not gain popularity.  This is beside the point.  The entire reason why Microsoft Surface tablets are purchasable today is that Steve Jobs' iPad team carried out the work and converted mouse-based, desktop software conventions to touch.  Why then does Microsoft not offer a touch UI mode for its Windows apps when they are shipped on Microsoft Surface tablets?

It is something this minor that can settle the Android and Apple app store debate: every person who writes application software knows that there nothing about a touch UI that dicates fusing it to an app store, as it is merely a reconfiguration of desktop conventions (e.g. making the buttons bigger for touch, removing the system menu at the top, or deciding not to make use of windows).  It takes expertise to this, which is not to be taken lightly, but the tablet isn't an altogether different computer with significantly different OS internals.

Windows apps on Surface tablets should automatically enter a touch-specific mode, loading separate touch-specific XAML files.  Microsoft can settle the app store debate by releasing a Touch SDK for Surface tablets, adding a touch mode to Windows apps.  The ability to add touch functionality to Windows apps is provided in a small way, but it isn't promoted as a comprehensive target like iOS or Android apps, which would make a coherent multi-touch situation that matches the experience of other mobile operating systems.  For the most part, Microsoft Surface is synonymous with desktop apps activated with touch.


