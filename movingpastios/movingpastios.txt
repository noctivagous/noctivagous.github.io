
Moving Beyond Apple's iOS to the Future --- (noctivagous)


noctivagous.github.io


The iOS UI Is Reductive And Limits Tablet Computers

For tablet computers, iOS (iPadOS) carries a major drawback: the simplest tasks are easy to achieve, but anything slightly complicated has to be specifically learned because it is invisible, offscreen, or buried in conventions.   The "user friendly experience" only describes the basics of launching an app, zooming inside a document, and scrolling.  After that, the system software interface features are tacked on, one after another.  The consequence is that a user can easily activate one of iOS's side features on accident, and this is a poor state of UI design.  But also, iOS is an undeveloped and prototype-level collection of user interface conventions in the first place, incomplete except for consumption of media.  Because it strives towards minimalism and commercial accessibility, it is nubby and dumbed down, limited for professional usage.  For the user's control of the tablet software, Apple, characteristically, insists on withholding from the user any complementary physical controls such as knobs, sliders, and buttons, believing that those were the problem in the first place.

If there are so many problems with the tablet UI, where does Apple's success come from?  Steve Jobs just knew what mattered at the time, to put together a tablet UI that worked for the hands and didn't require a stylus to navigate the interface. To accomplish that, he brought in the multitouch technologies that had been recently upgraded by researchers (see 2006 video: https://www.youtube.com/watch?v=ac0E6deG4AU).  This technology had been available for a a few decades but had been unutilized for consumer products.   The minimalist design ideology Jobs brought to the device was a secondary aspect, and it isn't the actual foundation of the iPad's success.  Rather, iOS is just a minimalist's deployment of multitouch screen and "gestures" on a tablet, artificially restricting it to walled-in "apps," which is an idiosyncratic and self-serving packaging of software.

In chronological order, Apple developed first a large, tablet-sized multitouch screen and then shrank that to make the iPhone. Then a few years later Apple released a multitouch interface device in its original, larger size as the iPad.  It is of course very popular and has also become a definitive reference for other companies, unfortunately.  Embedding a foolproof, kiosk type of user interface inside a mobile tablet can produce a lot of sales because anyone from the public can use a kiosk and thus anyone can use what iOS provides.  But it won't ever reach the depth of a PC's (a desktop computer's) capabilities.  That is the current problem, that no company has manufactured a truly finished touchscreen tablet UI, as Android is hardly more than a clone of Apple's products.  Application software will never be programmed on the iPad itself without the use of desktop computer accessories that make it behave like a PC.  Currently Apple's software development system (Xcode) is not available for iPads.  The stock UI of an iPad cannot accommodate it.

It's the design ideology behind the user interface that has caused this. Making a device blunt in its simplicity comes at a steep cost because eventually the apps that people want to make and use become complex, and software developers lack a systematic tablet UI scheme fulfilling their needs that could mirror desktop computer functionality.  When iOS is ideologically narrow, deeply determined to package bare interactive essentials as the environment for mobile life, it doesn’t possess any conceptual frameworks to accommodate complex usage of a tablet computer.  So necessarily there are PC trackpads and keyboard sold for these tablets, to turn them into PCs with screen cursors.   No screen cursor would be necessary if iOS carried a complete set of UI principles, but achieving that will involve breaking out of Apple's own ideology that shuns accessory physical controls on the device itself.  Whenever the tablet software is intended for serious usage, a trackpad basically has to be utilized.  Sony, by contrast, did not take this viewpoint about external controls and it is Apple's that has spread across society.

The theory of the PC was developed over a period of years by Douglas Engelbart, and later Xerox PARC, so a tablet's return to the desktop computer's conventions is inevitable when nothing comparable in depth has been developed for a multitouch device.  Apple and even Steve Jobs did not grasp this, as they were sure that the iPad could result in the "post-PC era" as soon as it was launched, indicating that Apple has always been terribly superficial when it comes to theories that produce the real computer experience.  The theory behind iOS isn't much beyond a set of raw conveniences to accommodate the multitouch “gestures” unveiled by researchers in the 2000s, serving to launch apps and press buttons inside iOS apps.  Apple's specific packaging of that effectively halted the industry's future development of touchscreen UI because manufacturers were only willing to copy Apple and Apple itself was holding to its own ideology that doesn't go very far.  For example, iOS is supposedly intuitive but it's difficult for the user to figure out how to open a file inside a specific application or how to manage the filesystem in a sophisticated way because the files are no longer the focus, just launching kiosk-like apps, for commercialized simplicity's sake.  The objective of making everything user friendly the point of being foolproof has resulted in an uncomfortable set of devices for several years now that can scroll, zoom, activate buttons but which severely limit engagement with the mobile computer's wide-ranging capabilities.

The evolution of the touchscreen tablet UI can begin by discussing what kinds of controls can be added to the device and what purpose they will serve.



Touch Operation Buttons

An object sits there on the tablet touchscreen, but what does touching it do?  In real life it moves according to physics.  On a touchscreen computer, the initial answer is vague, but generally "open" or "activate".  This is an operation, of course, and that is what differentiates the real world physics from the conventions of a touch on a tablet device.   On iOS, a single touch is defined as one operation: "activate".  To make the touch do anything else, beyond that function requires learning all of the complicated conventions afterwards, such as press down for longer to get something to appear.  The touch could be made to do lots of different functions, but right now it can only do "activate".

If several physical buttons were arranged in a column just to the left of the screen, they could to specify what a touch means when it occurs, and this will broaden the tablet's user interface a great deal and make the UI far more nimble.  Closing a window would not involve touching a close "x" that appears on an object but instead momentarily holding down the "Close" touch operation button on the left edge of the screen and then touching any part of window(s), panel(s), or object(s) that need to be closed.    When a certain touch operation button is held down, e.g. an "inspect" button, whatever object the finger touches will be acted upon accordingly (it will be inspected, it will be closed, etc.).  More than one object can receive that operation while the button is held down, making for a quick and capable UI.

To summarize: a touch operation button momentarily specifies what a touch/gesture means just before the touch occurs and then the physical button is released by the user.  The speed and straightforwardness of the interface improves: five objects can be removed from a page quickly by holding down a "remove" operation button, then tapping each of the five objects, then releasing the "remove" touch button that sits to the left of the screen.   This setup will allow the user to control software with a wide range of touch operations instead of searching for operations deep inside the interface through various convoluted maneuvers.  In addition, the physical touch operation buttons themselves can have their operations swapped out when they each carry individual LCD screens.  

Thus what is convoluted to achieve today on a tablet will not be, and what is out of reach for the average user who does not bother to memorize specific features will become straightforward for anyone when these physical buttons are placed onto the tablet device.  The key benefit when there are several touch operation buttons sitting to the left of the screen is that they can accommodate a wide range of touch objectives immediately, and this is a core of a new tablet device framework for user interaction.  A “generate” touch operation button will make the touch generate an object at the position of contact in an empty space (or it will generate a child object for the object that was touched).  To illustrate: five objects can be generated by holding down the "generate" touch operation button and then tapping the desired locations of the five objects.  For power users, some of these touch operation buttons can designated for custom-programmed operations and scripts.  Each touch operation button has an LCD screen.

Importantly, in this scheme the user will always know how to accomplish tasks because the touch operation buttons naturally work in combination with each other, being general in their function names, covering a broad range of activities. "Close" can apply to exiting or closing anything, usable for many types of objects on screen.  "Inspect" can apply to a variety of objects as well.   Just these two example buttons already work in combination: any inspection popover opened with "Inspect" can be closed with the touch operation button "Close", demonstrating that the operations work systematically already.  A user can know how to do all sorts of tasks without specific instructions when the provided operation buttons are helpful.  The current procedure for the user in iOS is to extract these functions (making close buttons appear or digging into the user interface to find the desired function) to activate them with touch) and this is no longer necessary.

More touch operation buttons:

-- "Teleport": instead of dragging, the first touch establishes what object is to be transported to another part of the screen and the second touch moves it to its desired location immediately, thus being teleportation.  So there are only two touches to accomplish what dragging does today.  This is much faster than dragging objects on a touchscreen and will be much more precise.

-- "Toggle" will turn a touched setting or object on and off, opposite its current state and this can apply to many circumstances.

Fortunately, the touch operation buttons reduce or even eliminate the need for what are currently called "gestures" because even a "Zoom" touch operation button will provide more precise control over the zoom as "Zoom" is being held down, allowing the user to go even slower than today.  Likewise, a "Rotate" touch operation button will provide greater real-time precision for the user. In the case it is still desired to have "gestures," there can be many types of swipe, rotate, or two-finger scrolling according to the touch operation buttons.

Touch operation buttons should serve to replace digging into menus to find the desired function, as is the current dynamic.  If the labels for these touch operation buttons are categorized and grouped adequately, an increase in the number of operations will not overwhelm the user but instead make it so the user does not need instructions for every task.



Opposing Touch Operation Buttons

In this vertical list of physical touch operation buttons on the left side of the scren, it might seem best at first to have just one column.   But often operations exist in opposition to each other (group --- ungroup,  add --- subtract,  generate --- remove, open --- close, show --- hide) .  Two columns of opposing operations might be preferable, doubling the number of available buttons sitting to the left of the screen because they are used together.  Touch modifiers opposing in purpose would sit side by side on each row. 

A window is closed with "Close," but "Close" can also be used to turn a switch control to the off position and close just about anything.  Pairs of buttons would have generic names: such as "Open" paired with "Close", which apply to many scenarios of all sorts.

"Expand  --- Collapse" is about unfolding and stowing away, a different state dynamic than "Open --- Close".



Mode Buttons Column

On the right side of the device's screen can sit mode buttons that are utilized by all apps.  These buttons do not modify the touch but instead the entirety of the app's mode or the mode of a certain context.  All apps running on the tablet would utilize these buttons as a primary means of interaction with the app, because all complex apps have states or modes.   Again, each of these buttons will be backed by individual LCD screens to give the software developer's flexibility.  The purpose of these mode buttons is to support anything that is a mode or state.  Most apps have various modes: viewing and editing being the most fundamental for apps that produce data of some sort.  Configuring preferences is another mode and when both the software developer and the user thinks in terms of "mode," because the device is built that way, there is no need for the user to look up specific conventions for opening the settings window.  (It's also possible to make a "Configure" touch operation button so that with a touch anything on the screen can be configured).

Other uses of modes include switching on and off special submodes of editing in the app.  The mode buttons should be able to accommodate anything needed involving modes, submodes, and states. 

The conceptual internals of an app and computing system are being represented on the outside of this device, around the screen.



Reaction Buttons Such as "Undo" and "Redo"

One could say that pressing "Undo" is a reaction to a mistake, thus this is a reaction type of function or button.  In an app, "Undo" and "Redo" act on a maintained list of user operation history that is usually out of view.  The two "Undo" and "Redo" buttons will not exist as touch operations because they act on the last operation done.  They sit next to the touch operation buttons because they undo or redo what was just done by a touch operation.

Reaction buttons can be said to be those which are used by the user quickly in response to an immediate change or request from the app. The periphery of the device is a suitable place to place these reaction buttons.  For example, if anything isn't right or should be exited, the user will want to press the "Escape" button in the top left of the device.  If something is to be confirmed, the user can press the Accept/Proceed key in the bottom right corner of the device.  Why include physical buttons when a person can touch the screen itself?  Because the less movement of the hands the better with respect to these types of user actions.



No Explicit Confirmation Dialog Boxes ("Are You Sure?") Needed Because of Implicit Confirmation Pathways

If an object is selected and it is of sensitive value for deletion (deleting it could create a problem if done on accident), then the following is how to delete it: the "Remove/Delete" touch operation button is held down and that object is tapped three times, with the object darkening with each tap.  On the third tap it is deleted. There is in this case no need to display a warning dialog box for the user that asks, "Are you sure you want to delete this?"  The pathway has been set up so that confirmation is implicit takes place through the action.



Scrolling-Averse System Conventions

Noctivagous holds the position that constant vertical scrolling is harmful to the user, is uncomfortable, and is a substitute for what should actually exist more often which is proper transitioning of content (sliding, for example) that is at all times correctly sized to the device's frame dimensions.  That is, content should generally be swapped in and out carousel-style or otherwise, faded in smoothly.  Content should less often be clipped and require scrolling.  Laid-out information should be prepared such that it is fitted for the dimensions of the device. "Forward" and  "Backwards" slides or fades in content within the same frame and nothing is moved into the frame of the device through scrolling.  This is especially applicable to consumption of information.  The web itself should be refitted in this way.



Separate X and Y Axis Touchpad Strips

A hardware feature that is considered but is optional for a Noctivagous tablet is a pair of X (horizontal) and Y (vertical) axis touchpad strips with pressure sensitivity.  They would sit above and to the left of the screen respectively and match the lengths of the width and height.  The Y axis (vertical) touchpad strip will sit on the left edge of the screen, sitting between the Touch Operation buttons and the screen.  These strips are available for all apps and the benefit is that they constrain movement.  Apps will be able to use these touchpad strips for horizontally and vertically constrained needs.  For irregular movements the screen itself can be used.  It is natural to select, for example, an app icon by lining up the x and y axes with both fingers and pressing down on the two touchpad strips at the same time.





NOCTIVAGOUS.GITHUB.IO




----


With touch operations, it is also possible that something is deleted not through the use of a "delete" modifier next to a "close" modifier but instead tapping an object twice with "close," with it darkening as a warning that it will be deleted will be the way to delete something.  Other modifier keys include "route" such as for opening a file in a specific application, because "route" is a very generic keyword that can apply to many situations.


Instead of "cut [out]","copy [from]","paste [on/into]" more like "extract","


  


NOCTIVAGOUS.GITHUB.IO