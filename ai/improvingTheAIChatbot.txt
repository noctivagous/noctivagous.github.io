Improving the AI Chatbot

INTRODUCTION

Steering AI responses towards satisfactory outcomes involves having it ask 
questions before answering or allowing it to mention a range of options for
a situation.  Then the user can be given an opportunity to revise or refine what
the AI offers to generate, within a sequence of interaction. Before taking action for the user, the
AI should be set up to collect information on important points, ranging from
large to small depending on the usage of AI. There are times when the AI can
interject to ask for clarification because some aspect of the project is ambiguous or incomplete
or perhaps the question asked by the user has other implications.
Many times, the user will instruct the AI to provide him or her questions.  It
is important that the user is able to ask it to do more than just ask for questions 
but also steer the AI towards the right kinds of questions for the situation and revise
the questions being asked.  This is the beginning of introducing genuine process
to the use of AI when currently the interactivity is structured as long responses from
queries.  Because they are not informed, the responses have to make many assumptions on 
many points, traveling far into any endeavor before the user has a chance to tell the AI 
whether it strayed from the goal of the request anywhere, on any point.  The user then has to pull the AI
back to what he or she wants, sometimes laboriously pointing out what wasn't wanted, all
because there was an absence of any questions at the outset.  If there are any questions, 
they are currently tacked on to the very end of a response. The AI is only doing what it
is set up to do, which is generate long answers with the limited information put in the
prompt.

The first move to make is an AI chatbot app that is provided a multi-purpose
interactive form file format that is tailored for each situation where the AI
will be involved in collecting preliminary information or asking questions to
know the needs and goals of the situation.


INTERMEDIATE RESPONSES I: CLARIFICATION QUESTIONS

Sometimes the AI chatbot user doesn't anticipate the different directions the
response could go, or the variations in contents it could have, especially when
asking about a topic that requires experience or is unfamiliar to the user. This
is definitely true for programming projects, where a seemingly simple query to
the AI can imply many possible approaches for the code. This points to the need
for an intermediate stage of response, which is a clarification menu that shows
up before the output is generated.  The AI would use it when the user decides.
Initially I would try mapping the ASDF keys in reverse order [FDSA] to a
4-option menu, so that the user can proceed rapidly just by pressing a key and
not get hung up by intermediate clarification questions. "For this code, for the
[topic] do you want F) D) S) or A) ?". This menu would only appear when the AI
determined it is beneficial.  It would not be limited to code situations but
rather is present at any time.  (The user could also insert shorthand into the
prompt for requesting varying degrees of clarification menus.) Pressing RETURN
will just move past the menu. When relevant, for more involved matters, the
clarification menu would carry multiple layers of questions (a tree of
questions) when the AI knows that a satisfactory response likely requires that
the user give multiple specification answers in advance. The key issue is that
the user does not always know the issues implied by a query, and so many pages
of text are generated in the chat session as the user tries to refine the
output, whereas often the AI could have asked clarifying questions.

For the shorthand, the user can dictate inside the prompt which issues of
specifications it wants the AI to ask questions about.  For a code project, the
user might type [specq: programming library, variable naming. depth:4,
quantity:4] and then get questions from the AI in these areas in a quantity of
questioning level 4/10 and tree depth of 4/10. Thus the user can be proactive
about the output from the AI.

What this means is that for any prompt there will potentially be two API calls:
there will be two when the AI has determined that clarification is needed for
the prompt while there will be just one, the response as usual, if it doesn't
need clarification and no menu will appear.  The AI will be given a
clarification frequency value ranging from 0-10, with 0 being never ask for
clarification and 10 being always ask for clarification.  The default is 5 which
means ask for clarification whenever the prompt from the user is vague or there
is information the user did not anticipate.  These instructions about the
clarification menu will reside in the system prompt.  When the client receives a
response with 	</aichat:clarification-response> it will render the menu and
options accordingly and also handle key and mouse down events for the menu
items.

The user should be able to make the clarification response longer or take it in
different directions, because at the time the clarification response appears the
user will know other implications in the project or query.

There is another case where, if the user makes an observation and sends it to
the AI, such as the user is correcting the AI's response, the AI should, just
like a person, make a statement that validates the comment but then also show a
clarification menu for what to do next.  For example: "What I see in the code is
that lines 100-123 in api.js are in the middle of [computer task] and it isn't
the place to detect [some kind of state in the program]. "  The user might not
have an answer as to what to do right there but the AI can express agreement and
then provide some options as what to do next. This situation also points to the
need to have subqueries, or subprompts that are present only within the
clarification menu so that the user can refine the plans of the AI before the
next step.  Chat sequence steps will then have hierarchy. The overall one is the
one today and it is flat.  But in this example there is a possible nested layer.

Alt+LeftClick/Opt+LeftClick on a clarification option will open it in a tab of
the current session, which is in keeping with our section below, "BRANCHING OFF
AND SIDE QUERIES." On mobile and desktop there will be a small button affixed to
the side of the button that is a symbole for "branch off" that puts it in a tab.

We also want dropdown menus to happen, such as "Programming Language : [Python]"
which will be represented by
<nct:configoption-dropdownmenu><option>Python</option>
<option>JavaScript</option></nct:configoption>.


INTERMEDIATE RESPONSES II. MANUALLY-TURNED ON AND OFF: "SPECIFICATION
QUESTIONNAIRES" (WITH SWITCH NEAR PROMPT TEXT BOX)

The user may choose to deliberately write initial requests or questions and
then, when the mode "Specification Questionnaires" is turned on with a checkbox,
the AI will generate specification information collection forms or
questionnaires, of a length determined by the user.  This allows the user to
have the AI collect all of the implications it finds for a project.  It has
multiple uses.  There are a lot of times when the user knows that there are
aspects of the projects that will come up that the AI will know about that the
user might not consider.  The user might make some objective with slightly
ambiguous implications that require distinctions, issues which the AI will often
recognize and bring up.

Additionally, there is the case with these questionnaires where the AI will
gather feedback from the first screen, process it, and then have new questions
to ask based on that information. So then it will generate a new questionnaire
screen in place to get more information. In this way it will have a much better
impact on the final result than the user pushing back and forth with AI chat
responses like right now.


An example:

Q: Write a sort algorithm R: What type of algorithm, language, and instructions
do you want? -- Algorithm Type - Specify which sorting algorithm, like bubble
sort or quicksort. -- Programming Language - Choose a language, such as Python,
Java, or C++. -- Other Details - Provide any additional preferences, like
complexity or examples.

Whether it is automatic clarification questions that appear or specification
questionnaires, in both cases a breadcrumb or history can be stored, including
in the form of sequential tabs from left to right, so that the user can explore
different aspects of the same topic.


INTERMEDIATE RESPONSES IIb. SAMPLE SELECTIONS MENUS

For tasks such as translation, the output can have different variations
possible.  The user can ask the AI to give a list of sample output, allowing the
user to choose what style, for example, will be applied. This can apply to color
schemes, design layouts.  The AI can give previews of different solutions it
will make for a code problem.


INTERMEDIATE RESPONSES III: MODIFIABLE OUTLINE AND PLANS

The AI provides an interactive form, with textboxes underneath each section and
editable outline points, that indicates what it plans to do next.  The user then
gets a chance to make changes.



BRANCHING OFF AND SIDE QUERIES

One issue is that there are times when the user wants to know more about a
subject found within the generated text but not necessarily interrupting the
stream with a query about it (which will also affect later responses).  There
can be a mode provided in the AI chatbot where sentences and paragraphs can be
clicked on, which opens up accessory or marginal tabs for exploring topics in
the original generated response.  The output will be associated with the main
chat sequence but sit to the side or behind it.  In other words, the user should
be able to branch off from the initial chat session and return to it.  The AI
chatbot can certainly have other tools too, like built-in dictionaries and the
ability to add categorization tags to chat sessions.

From a menu of options (e.g. clarification menu), once a user selections one of
them, the result is a new tab.  The user can then go back to the first menu and
select a different option and a tab will open next to the previously generated
tab.  Each time a question menu or interactive form shows up, it will occupy its
own tab. This allows the user to go traverse the AI output, backwards and
forwards.

"Send to Scrapbook" button on any chat response allows sending the contents of
that specific response to the client's scrapbook.

The user can choose to reply to the responses have have the AI respond to the
right, in place, with tabs of the response, or they can go down.


BRANCHING OFF AND SIDE QUERIES II: TROUBLESHOOTING DURING TUTORIALS AND GUIDES

There are times when, although you are following steps inside a chat response,
you need to break out into a side tab where you investigate a problem and come
back. One of the steps in a tutorial might not go as expected and you need to
troubleshoot. This should be able to take place within the same chat sequence
and not add another question beneath it.

For a chat response, you should be able to ask a question underneath a paragraph
or section and have it 1) answered there and then it can be collapsed within its
accordion control 2) answered in a side tab.





HYPERTEXT FEATURES

When processing text, the AI might have another agent activated that adds
metadata features to the text.  These might include definitions for certain
terms, especially for translated materials, and contextual information.  The AI
can generate hypertext features, assisting in the use of text.

When the question mark cursor is activated during hypertext mode, clicking on a
term will open up a bubble explaining it, querying the AI for the popover.


INTERACTIVE FORMS

The AI chatbot could also respond to the user's code (programming) query with an
interactive form where it has generated an outline of what it plans to do for
the project.  Underneath each outline point are text boxes where the user can
change or refine the AI's plans.  At the end of the form, the user can then
choose to either receive an updated interactive form or it can have the AI
proceed.  In another variation, the AI would include questions inside the
outline and the next step would be the interactive form that the user reviews.

This also shows that the interaction with the chatbot that the user has with the
AI doesn't have be at the end of response and separated from it but can be
interacting with controls and textfields that the AI embedded inside it.

The AI isn't always going to generate what you want. The AI chatbots could
certainly implement intermediate steps where the user can get an overview of its
plans, especially for programming projects, and then refine them iteratively
before anything is generated.

INTERACTIVE FORMS II

The AI is given a list of tags that it can send to the client to represent
controls (for example,  <aichat:textfield>) and the AI can send these inside a
type of chat response that it uses as an intermediate step, to gather
information before generating the final output.  These include 1D and 2D sliders
(<aichat:slider>), multiple choice lists, color pickers, checkboxes, etc.
Replying to the AI after it embeds these in its response is then about
interacting with the chat response, not typing in the followup chat textbox as
is the case for everything now.  At the bottom of this interactive chat
response, the AI will have generated different submit buttons according to the
situation.  The AI can also, after receiving a submission, update the contents
of this interactive form in the place where it sits instead of generating a new
one underneath.  (It would send a command to the client like
<aichat:updateinplace>). What this implies for chatbots is that the the AI can
generate different types of chat responses (data types), some of which include
embedded interactive controls that allow it to tailor its output extensively in
tandem with the user. It can get a lot of information before carrying out the
request.  In a more advanced version, the interactive form that it generates
will be running code so that the AI can collect information from interactive
diagrams (https://github.com/mermaid-js/) that the user modified.

The interactive forms can provide just a general outline of the AI's proposed
plans or they can include sample details of how the AI intends to execute the
task, such as sample text or code.  In a translation effort, for example, the AI
can generate multiple options for the style of translation.

Other tags include <aichat-control:curveeditor>, <aichat-control:sequencer1d
cols="8"> for a drum sequencer, <aichat-control:sequencer2d rows="5" cols="8">
for a musical note sequencer. In the software, the user would be able to add
custom control tags for custom use.


TAGS

A title list allows titles for each list item and the controlView determines how
it is displayed.  Default shows up like ul li with the title preceding in bold.
Instead of ol, use features="numbered" and the features has multiple
comma-separated properties available. What the programmer can do is make new
controlViews for different outputs in a .controlView file that uses provided
callbacks. The options attribute allows hiding and showing of different list
contents.

<aichat-control:titledlist controlView="tabs" features="numbered" options="">
<!-- view="list", view="accordion", view="grid" -->
<aichat-control:titledListItem>
<aichat-control:title>Title</aichat-control:title>
<aichat-control:description>The description for the list
item</aichat-control:description>
<aichat-control:contents>Contents</aichat-control:contents>
</aichat-control:titledListItem> </aichat-control:titledList>




INTERACTIVE FORMS III

Every response from the AI chatbot could make use of an interactive form, which
would reduce the page length of any chat session by allowing modification and
feedback in place by the user, who decides whether to update the response in
place by using the corresponding submit button at the bottom.  The alternative
is that the user just uses the existing text box to respond as usual.  This
means that every response would make use of some of the control tags sent to the
client (e.g. <aichat-layout:flexboxcontainer>) so that the AI produces complex
layouts with interactive controls.  The issue is that the AI usually can only do
one thing well at a time, so it might require a second request to reformat the
response within the tags.


INTERACTIVE FORM TAGS REFERENCE

<!-- gui controls --> <aichat-gui-control:button> <aichat-gui-control:textfield
dataformat="plaintext"> // richtext <aichat-gui-control:numberfield" > //
integer, float <aichat-gui-control:checkbox> <aichat-gui-control:radiogroup>
<aichat-gui-control:radiobutton> <aichat-gui-control:slider-1d>
<aichat-gui-control:slider-2d> <aichat-gui-control:slider-circular>
<aichat-gui-control:colorpicker> <aichat-gui-control:sequencer-1d>
<aichat-gui-control:sequencer-2d>

<!-- layout --> <aichat-layout:flexboxcontainer> <aichat-layout:tabview>
<aichat-layout:tabview-tab>


<!-- external data --> <ai-chat-document-viewer type="pdf"> // for fetched
documents <ai-chat-iframe url="">

The client will construct the various interactive components on the fly based on
the provided tags, amounting to mini-applets in any given response.


CUSTOM PARAMETERS (AND ACCOMPANYING GUI CONTROLS) FOR AI CHATBOT

The clarification frequency is a parameter controlled by a slider in our AI
Chatbot Lab, for determining how often the AI interjects with an intermediate
clarification menu for vague questions or cases where there is information that
could assist forming the answer, including when the user could not have
anticipated that the question would be highly beneficial.  The user should be
able to add custom parameters and matching GUI controls like this of his own
that affect the output, such as tone.  There can also be presets from a library.

In settings, these can be defined by the user, represented by this tag:
<aichatclient:customparam min="" max"" effect="">



DESIGNED CHATBOT OUTPUT

The first call to the LLM only generates design layout placeholders in a
wireframe containing a sentence in each container about what it should contain.
Then the wireframe contents of this response are sent with the initial prompt to
the AI again to fill in the containers with text.


THE AI CHAT APP HAS DIFFERENT LAYOUT PRESETS FOR DIFFERENT USES

Quick interaction for just getting an answer.



THE AI CLIENT RECEIVES DIFF COMMANDS INSTEAD OF REGENERATING ENTIRE FILES

A problem some software developers mention with the AI chatbots is that while
working with them they will write over their good code or remove sections that
were working.  This has to do with the AI having to do too much in one query
because it regenerates the entire code file from scratch rather than making
changes to specific sections.  When it regenerates the code, it can't handle all
of the different aspects from the beginning and omits things that were there.  I
think there is potential in having the the client receive a list of diff
commands, with insertions and deletions being generated by the AI instead of the
entire file.  To do this, it might be beneficial to generate unique IDs for each
line number.


PLANNING AND PROJECT TRACKING WINDOW

This represents a broader usage of the LLM within an app beyond simple back and
forth chatting, where the app integrates other aspects. Persistent planning and
outline window or tab for projects, optional. Updated as project progresses
while also editable. A maintained tracking window.

A similar window for writing code is one, driven by an AI agent, that maintains
a navigable diagram (MermaidJS) of the project, where clicking on an element
will go to that section in the code.


CLIENT-SIDE COMPONENTS

Webview and code editing in two different tabs for updating the result.


PHILOSOPHICAL COMMENTS

People are generally focused first on what the AI can do for them directly, how
they can gain from it.  The software developers think this way, and from this
mindset comes their treating AI as a way to automate or achieve without effort.
They make products around these limited goals.  This is a dead end, because when
software is built from this mindset, the AI can never play the role as a tool,
only carry out tasks as a servant.  Those are two different states. A better
starting point for the software developer is to identify what the gaps in
capability and knowledge are between the person trying to use AI and what the AI
itself can do and has available, for any task.  Let's say the person wants to
write code for a project.  The AI knows all the ways the person's code can be
written and how his or her project can be put together, as well as the
implications of any given move he makes, at any given step.  It's extremely
capable in this respect.  The person doesn't have that range of knowledge, so
why is he dictating to the AI step by step, as if he the one who knows?  An LLM
has inconceivable amounts information available on any given topic, and if you
ask it a question it has to select  from its information and summarize.  This
means the software developer didn't give it a chance to tell him what it knows
before he gave it instructions and he goes down a narrow path of reacting to its
responses (ChatGPT today) and struggling to get what he wants.  Then he can't
truly make use of the AI as a tool, because he isn't acknowledging what it has
versus himself.   In many cases, the available instructions should be generated
by the AI itself.   You react to those and thereby continuously inform the AI
based on what it tells you.   This is genuine interaction with the AI and it
will produce superior results.  You have allowed the AI to play its role as the
thing that knows, and you equip it with preliminary information at any given
step from options it generated from its knowledge.  The AI helps you instruct it
by giving you relevant instructions to give it for the current situation or step
in the process.  The AI has to be allowed to interject under certain conditions,
such as if the person's request is too vague and needs clarification, because it
knows more factual information.   By allowing the AI to interject, and telling
it how to interject and under what conditions, it gets valuable information for
the user's next step based on its vast collection of knowledge.


NOT MUCH CAN BE PROVIDED BY THE USER IN A PROMPT.

There is a noticeable limitation in the chatbot software's usefulness despite
the AI's immense power, because the chatbots don't ever provide one or more
preliminary questions before responding to queries, which could give them a
broader picture of the user's goals.  As a result, they can't know that much
about what you are trying to get from them, apart from what you type out one
prompt at a time.  And typing out prompts in reaction to generated output is not
an adequate protocol.  Why not offload the work of informing the AI about what
you want onto to the AI itself?  It will be better this way: the AI can ask a
question, provide some pertinent options, and get some input before responding.
The AI will have more information that it needs to respond satisfactory, which
you can't be expected to put together by yourself in a prompt.  The AI can
present some preliminary questions accompanied with menu options, mentioning
aspects of the original query that a person hadn't considered.  Then the path
the interaction with the AI is different than the current chatbot.


DISCUSSING THE CODE-GENERATING EDITORS

On the one hand, it is impressive to see code inserted into a project, entire
files generated from AI prompts, and the previously challenging-to-assemble app
running in 10 minutes.  On the other hand, though, these   code-generating
editors are still lacking process for software development, which may seem like
a minor point when they can do so much.  Process matters: when the feature
omissions are there to accommodate what software developers do outside of
writing the actual code, an app like "Cursor" becomes about constantly catering
to the AI prompt text box and chasing down the quirks and idiomatic behavior of
the automated code IDEs.  These IDEs do not make, as the center or anchor for
the software developer, a prominent list of the programmer's todo items or the
project outline.  This would exist inside a dedicated pane (expandable to a
window), where the code IDE checks off list items that have been accomplished
and has expandable aspects of tasks to be done.  As long as the contents of this
outline in this pane is always editable by the user-- since the user may need to
change aspects of the project midway-- it will assist in the unpredictable
process that is making a software project from scratch.  Also, as long as the
outline pane is synced with the contents project at all times, the AI agents
will play the right kind of role.  Software development isn't just about writing
code but also referring back to the big picture, something the IDEs have never
truly assisted with.   It becomes even more important now that there is AI
becoming the focus of the job.  This outline pane can have multiple tabs for
multiple parts of the project (appearance, data models, etc.).  It should be
more than just a to-do list editor and can embed other capabilities of AI to
interact with the software developer, such as displaying menus of options for
paths the software developer can go down, interactive menu sequences,  a sidebar
for pointing out considerations and giving suggestions for macro and micro
issues of implementation.  In aspects besides an outline pane, such as working
with the code files, there are areas for process improvement as well.  These
IDEs aren't providing or emphasizing action buttons that automatically document
the code or keep them in sync with a documentation file.  Supposedly that is up
to the software developer but all good practices like this should be included as
options in a development environment and made convenient too.



UTILIZING PRELIMINARY DIALOGUES / INTERMEDIATE MENUS FOR GUIDING CODE GENERATION
AND EDITING FOR DIFFERENT LEVELS OF THE PROJECT

When a code editor assists a person in generating code, what is left out of
these code-generation editors is an acknowledgment that there are different
approaches to code and corresponding guidelines that the AI agents can follow.
This is opposed to the current situation where it just write code for a request.
Should the AI, when generating the code and analyzing the project, move it
towards procedural, declarative, or some other programming philosophy?

As is said above, what is important is that the user be provided options
generated by the AI to address these different issues.  It's true that the
software developer knows of these aspects of a project when they're mentioned,
but the AI can offer a a menu selection.  These preliminary dialogues with the
user can help it can establish its overarching approach to the software project,
in a way where the software developer is providing input at the same time, not
just answering questionnaires.  How should the AI generally style the appearance
of the app?  Does the user want to set up some design guidelines?  The AI can
make suggestions for this that the software developer wouldn't have brought up.

Approaches for the large scale overview might involve the file folder
organization, how the AI code agent will tend towards dividing code into files.
By what principle does it suggest (or decide to make) a new file for a section
of code?  Is it monitoring the project to tell when it is time to suggest to the
user (or do it on its own) that a certain collection of code should be
reorganized, divided into other files? This is currently entirely up to the
software developer.

Approaches for the medium scale might involve the code files and how they are
written and organized to relate to one another.  Approaches for the micro scale
would include how a line of code is written, how variables are declared, how .

Though the user could hypothetically do all of these by himself or herself, the
reality is that they should all be managed by the AI.


NOTES SECTION 1.

A preliminary response dialogue or intermediate dialogue is an interactive step
generated by the AI in response to what the user asked.  The goal is to go
beyond just answering directly what the user asked in a prompt and treat the
prompt as a starting point that requires the AI's involvement, where the AI
generates forms. It can appear in multi-step form, with a decision tree carrying
the paths the user can travel for the single question or multiple questions
within an interactive form. In another situation, it can just be a single
question carrying a few different menu options. Some may have controls.   In the
case that a feature called "automatic clarification dialogues" is turned on, the
AI recognizes when the user's prompt, without providing more information, will
lead to an unpredictable outcome for the user's goals.  This includes cases
where the user may not have anticipated aspects implied by his or her own query
and the AI can.  When manually triggered by the user, this kind of dialogue
allows

Preliminary dialogue sequences can take on different forms and serve different
purposes, ranging from just a single question to generation of actual GUI
controls for the AI to gather complex information.  For collecting information
for a project before proceeding, the AI may generate one or more pages of
interactive forms.  Importantly, these forms can be modified by the user, and
not necessarily in the current static way.  AI can be integrated anywhere, and
the modification of the interactive form is a sub-location where the user can
once again be in dialogue and collaboration with the AI to shape it, so that the
questions asked reflect the project and are informed.  He or she does not have
to leave the interactive form to revise it with more information from AI, in
other words.

In future AI chatbot software, selecting one of the options from the provided
menu of options opens a new tab within the pane.  You can return to the original
menu pane and open another option in another tab.  You will be able to interact
with an AI-generated menu, including regenerate it in place with revisions.

You will be able to customize the AI chatbot interface and add different submit
buttons beneath the main prompt. "Get Questions First" asks a certain number of
questions (configured by you) to get the kinds of preliminary messages that
serve you want. Another button might be "Get Interactive Form" and you use it to
trigger an interactive form.


NOTES SECTION 2.

The value of having an intermediate step that provides suggestions is the AI can
be asked to suggest different, existing practices that In particular, it can
offer to utilize different kinds of interactive forms based on the
circumstances, if the user does not have a specific one in mind. For software
development, one kind of interactive form that can be generated by the AI is
called a Product Requirement Document (PRD).  The user does not then have to
labor to come up with all of the different aspects of the project; the AI can
provide all of the sections and the user leaves blank what can be filled in
later. If this type of document is monitored and maintained by the AI, for
example, during software development, it can serve as the anchor for the
project.

NOTES SECTION 3.

The chat response from current AI could certainly provide presentation modes
where the user's reply occurs within the actual response rather than in the text
box underneath. The arrangement of a multi-page output of text and then
responding to it only underneath it is clumsy compared to the user addressing
each point and question within the response. This might entail activating a mode
for the chat response where the user can interact in a way similar to word
processing annotation.  Perhaps this would involve inserting text boxes
underneath sections.

The chat response text from current AI chatbots could provide a mode where the
user is able to reply inside the actual response rather than in the prompt text
box.  The user can address each point and question asked within the response. It
is a computer after all, so the user can interact with the output if such a mode
is provided.  Once this mode is activated by the user, it might be similar to
word processor annotation.  Alternatively, this could involve the user inserting
text boxes underneath sections or paragraphs.  In any case, the output generated
by AI should be more editable in place as opposed to requiring replies by the
user that initiate an entirely new body of text each time.

This does relate to the issue of the chatbot software needing to reduce
scrolling, since lots of text is often generated.  The AI can produce tab views
for the entire chat response, dividing sections text into tabs that open on
hover.

Asking questions about the generated taxt, as we said in "Branching Off," can
take place with popover menus rather than generating.

A related note, on a computer documents should be able to be saved after viewing
with accordion sections collapsed, instead of just reflecting full pages print.



VARIATION OF INTERMEDIATE STEP: COLLAPSED ACCORDIONS FOR RESPONSE SECTIONS

After a response, the AI generates a series of collapsed accordions with titles
and short descriptions.  They are all editable before opening with the "Edit"
button. After opening one of them, the AI starts to generate a response for that
section and fills it in.

The AI chatbots generate lots of information that has to be skimmed over.  The
first thing that AI chatbots (e.g. ChatGPT) should do is offer the user the
option to get responses formatted as collapsed, empty accordion list items with
section headings.  This way the user can choose which sections to open and when
the section is opened only then does the AI generate the information.  This is
especially relevant when asking the AI about programming matters, where it will
generate steps to do things before they were really asked for or before the user
was ready, such as step-by-step guides, which may be useful in the future but
shouldn't necessarily be included automatically.  The next steps the user would
take after getting the information should be should appear as collapsed
accordion list items first.



OUTPUT COMMANDS, SAMPLES DIRECTED TO USER-DETERMINED UI LOCATIONS

In the AI chatbots today, when the companies are trying to test out which
version of a response is accurate to improve the AI, they generate different
panes and ask the user to choose the better response.  This is something that
should be available to the user, to direct the AI to generate responses within
multiple panes.


AI's APPROACH TO IMPLEMENTING LARGE SOFTWARE PLANS

As people work in software development, new considerations and issues crop up as
the project progresses, unlike the standards established for building design and
construction and the relatively straightforward expectations in that industry.
Those too can have back-and-forth periods, but most of that is in the site
planning and design stage.  With software development, the implementation and
design can be said to be merged, unlike building construction. The computer code
documents can be said to be the actual program, even though there is a
compilation process that occurs to turn them into it, which means that every
line of code inserted is a portion of the machine as soon as it enters the code
base. The design stage and implementation stage are as if merged even when the
diagrams for the program are plotted in software, because those diagrams
represent future code. The components used to make a consumer appliance are
physical and the materials are different (the plastic enclosure vs. the PCB
board).  The components used to make a computer program are of one uniform
quality, computer code, even though within that material, computer code,
different types of components are made.  They are not different in the same way
that the button on electronics is different from the case, which is different
from another part.  They try to be that but are doing it all out of the
substance that is computer code.

When told to implement something relatively big that isn't completable with one
query in today's LLMs, instead of just getting started and overwhelming the
software developer with plans and generating a lot of code for just the first
aspect of the software project, which also has to be reviewed, AI should focus
first on what is sustainable for beginning that project in collaboration with
the AI.  The AI should be equipped with the right tools as well (intermediate
dialogues to get information, in different firms) and thus become able to be
sensible about how to approach a big project, suggest a beginning with a
scaffolding or demo of a small part of the large project.  Or alternatively,
make multiple small demos for different aspects of the large project then at the
end the AI can work with the software developer to tie them together.
Suggestions of course appear through intermediate steps, working in
collaboration with the software developer.  Right now the software developer
just asks the AI for something, gets something to happen, looks to see how well
it worked. But this isn't linked into an overarching plan.

AI needs to be able to know how to approach large projects because even a small
app is usually big relative to what it looks like.

Maintenance, upkeep, organization are activities the AI agents can be made to
supervise.


THE AI's APPROACH TO A DOCUMENT

If given a document like this one, and discussing what can be implemented, the
AI should look for what it can handle first that is inside one subject. What it
does instead is try to make a general approach for the whole of the document, an
approach stemming from a summary.  While that is something desired for the task
of summarizing, for implementation the AI would be better off making an overview
of the document and then focusing on one aspect that it can handle at first.


PARALLEL ACTIVITY RESOURCES

A running summary, outline, or specification document updated in parallel with
the main chat sequence, to the side of it, which itself can be edited by the
user and paragraphs and sections modified with AI.  Multiple resources can be
updated alongside the main chat stream, the activity of the software developer.
Accessory windows and tabs can reflect the main interaction threads.


EXAMPLE USE OF AI: AMAZON'S "EXPLAINER" TOGGLE FOR AWS

When this toggle is enabled on the AWS website, text that is highlighted has a
popover appear and text is generated to explain it, using API calls to Amazon's
own LLM.

In an app or a browser extension, this might be set up in a different way, where
a command shortcut turns on key-click (bimodal control theory) highlighting.
Paragraph, sentence, or word highlighting can occur without the use of a toggle
and the key-click UI is a preferable way to go since it can accommodate all
cases, from text highlight, paragraph highlight, to word popover.


DIVISION OF INFORMATION IN THE CHAT RESPONSES: ACCORDION SECTIONS FOR CHAT
RESPONSES, KEYBOARD SHORTCUTS TO EXPAND/COLLAPSE ALL SECTIONS.

The chat responses themselves probably should embed accordion GUI controls for
each section, toggling contraction and expansion of their sections.

Some responses should be divided up into tab controls so that scrolling isn't
necessary.


ASKING ABOUT WHAT TO ASK, PRELIMINARY INTERMEDIATE QUESTION: "DO YOU WANT ME TO
ASK ABOUT [1,2,3]?"

In one kind of intermediate menu, you can check off the kinds of questions you
want the AI to ask you about the topic.


GETTING THE RIGHT INFORMATION FROM AI IN A RESPONSE

In a chat conversation with AI, I first asked, "Can you use OpenJDK as a drop-in
replacement for Java?" and the answer provided was yes with explanation, and
then I asked, "So if you want to get started in Java and write apps for free
[without the commercial Oracle license], what do you do?"  And I got an entire
page of text that started out with installation instructions for OpenJDK.
Terminal commands appeared three bullet points down.

That was of course too much for me from the outset, as I was not yet ready to
install and I was still gathering information.  An intermediate menu might have
said, "Q: Where are you coming from? A: [a new user from another programming
language] [a Java developer] [etc.]" and Q: What do you want to know first? [An
overview of getting into Java through OpenJDK] [An overview of installing Java
through OpenSDK] [Considerations using OpenJDK]".  As long as it is fast to
answer these intermediate questions, with keyboard shortcuts on desktop or
buttons to tap on mobile devices, they won't pose a problem and will make the AI
chatbot much better.  At all times, the AI should consider where the person is
coming from in his queries, but it has to ask needed questions.

The AI tried to dump everything helpful that it could in the chatbot response,
then it was my job to go through it all.  Again, if that is the type of response
then it would be organized into a tab control.


Take the sample question, "How would JavaFX and writing Java work inside
Cursor?" You get as a response a summary of what the two things are when it
could ask whether you know and give a better response for your background. 1)
Have you used Cursor?  Yes 2) Have you used JavaFX?  No


FORMATTING FOR TUTORIALS AND STEP-BY-STEP GUIDES

The AI chatbot front-end should treat tutorials and step-by-step guides
differently by allowing multiple views.  By default, I would show them in a
slideshow layout, with back and forward buttons.  Then, in any given slide the
user should be able to ask questions and the answers appear on that slide as a
new tab.  If the step's information takes up more than one slide the same tabs
would appear on all of the slides for that step.


WORKING ON A PROJECT IN CHATBOT VIEW WITH PREVIEW

The current way of working with code is to have the entire project opened (the
directory of files) and an AI assistant placed to the side that receives
instructions.  Still in this case the program has to be executed or reloaded in
a separate window after making changes, and the output is separate from the
process of generating and editing the code.  Another way would be to use the AI
chatbot chat format for the center with the responses containing portions of the
running program.  In this setup, the software developer works on one portion of
the program at a time (such as a Settings window) and sees the results inside
the chat.  What exists inside the chat will exist separate from the project.  It
won't be pushed to the project files until the software developer chooses.  This
is to say, the AI will maintain temporary forks of the project for the purpose
of the chat where the the output for that one component is revised and viewed.



AUTOMATED CODE EDITOR : MAINTENANCE OF DIAGRAM FILES (MERMAIDJS DIAGRAMS) IN THE
CODE EDITOR

The code generating editor can be told to maintain diagrams of code files, using
all of the available diagrams in the MermaidJS library (using Mermaid CLI:
https://github.com/mermaid-js/mermaid-cli).  So while it may not always have a
1:1 mapping of the code to the diagrams, it can get close enough to read the
diagrams before looking at the code files and when it makes code changes it can
consider them relative to the diagrams, which provide an overview that the text
of the code files cannot. An extension for the code editor can provide a viewer
of those diagrams for the software developer.  When editing any file, the AI can
look at the diagram first so that it knows what it is about. When the code file
changes, it must always update the diagram.  It will maintain other diagrams,
such as one for the overall view of the app (app.mermaid).


Planning Diagrams vs. Code Diagrams

There are to begin with two diagrams used for the program's files, the first
being the plan, which can undergo changes in keeping with the unpredictable
changes that take place as issues are encountered or refactoring occurs.  The
second is the implementation, which is maintained, every time the file is
modified, to reflect its contents in a diagrammatic form.  So the first is for
planning while the second is for knowing what the file contains.  The level of
detail in either one does not have to be granular but serves as an entry point
for the AI to understand the project's plans and what has been implemented so
far.  The goal is to have the AI use the planning files for its own planning and
let it contrast it with the implementation so that it knows what needs to be
done. The two diagrams are maintained for the AI to perform planning and gap
analysis.


The App Overview

It begins with the Project Overview file, which is present in any project,
whether it is an app, plug-in, extension, etc.

projectoverview-plan.mermaid - for Cursor to make a diagram that is an overview
for implementing the app (app.js)

projectoverview-implementation.mermaid - for Cursor to make a diagram that is an
overview of the code implemented.

The plan will have portions in it marked for the future.


file1.java file1-java-plan.mermaid file1-java-implementation.mermaid




AUTOMATED CODE EDITOR : PRODUCT REQUIREMENTS DOCUMENT + EXTENDED TECHNICAL LIST
(TWO LEVELS OF DETAIL FOR PLANNING)

The product requirements document gives an overview but for the AI and the
person to keep track of the more technical aspects, there can be a more detailed
companion document called Extended Technical List.


USING AI, CONVERT THE PRODUCT REQUIREMENTS DOCUMENT TO AN INTRODUCTORY GUIDE TO
USING THE PRODUCT.

We can approach a Product Review Document in terms of a full documentation
guide, but we want it to give us more as if it is specifications as well.  Then
we can get a view of the project that truly guides us (and the AI).

Have the AI rewrite the PRD so that it is a full document guide to using the
finished product, with an introductory section, providing example code and steps
if it is a programming library for example. This provides another perspective
for how the product will work, revealing gaps and incomplete aspects, and other
issues that need to be addressed by the software developer before handing off
the PRD to the AI, anticipating how it will be used. For the AI, this document
can be paired with the PRD so that the AI knows both ends of the situation and
gets a better picture of what the product should look like as it works on the
code.

This is because the PRD gives an overview but doesn't capture how the product
exists and a complete specification document doesn't do that.  If on the other
hand the AI is generating documentation for the product, the software developer
will see what needs to be added to the project and what needs to be worked on
and the AI can work backwards.

What is also important is that an introductory guide is addressing the end user
with an overview instead of just the person or persons developing the product.
By giving a picture ahead of time for how the product will be used and received,
it shows what needs to be done.  The description of what the product is is more
helpful when it is explaining it to the intended audience.  From the very start
the product will be integrating how the user uses the product instead of simply
detailing what is wanted for the user to have. Here you can explain the
philosophy of the product and what purpose it serves in the current tech
environment, whereas in a requirements document you will not.  It gives an
opportunity to provide a tech industry historical context and accompanying
explanation.

An introductory guide goes through a step-by-step process for using the
software, which shapes it form as a product.



GETTING THE AI TO PRODUCE SAMPLE USES AND FILL IN THE FEATURE GAPS

In the following case, the NCT Framework is still being flushed out and not all
of the necessary tags have been made.  When asked to make the framework, the AI
will do some tags and features, but it is behaving from the forward approach and
come up with tags that it thinks the framework will need to have.  That isn't
giving it any real world examples.  So what we can do is have it approach it
from the other side, that of a real app.  We start with the tag that doesn't
exist yet, which is <nct:app></nct:app> and ask it to make an example app within
our current system.

PROMPT: "Make an example app with <nct:app></nct:app> as the enclosing tag. You
can make any tags up that you need and any conventions necessary that fit into
our philosophy."



AI-GENERATED CODE VS. CRAFTED PRODUCTS

AI is not in a state where it can naturally help you craft products. What it
does is carry out tasks when asked. The AI gives the impression that it could
help you make whatever Apple has done with an iLife app in a flash.  It doesn't
do that by itself, and if you ask it to do things it just generates variations
of what has already existed in its training data, until you instruct it
otherwise.  This is a key distinction, which is that it can definitely make
high-quality utility code for utility apps.  It can definitely generate code for
certain portions of a crafted product when you make a request.  But as for
crafted products, like an iLife app (iMovie, Final Cut Pro), they have to be
built up as a whole, working on many different aspects in a custom way, moving
across different levels of them back and forth, so that they each fit into the
overarching philosophy and goals that the product carries.  Doing that isn't a
simple matter of delivering prompts at each step, in actuality.  This is a big
reason why we haven't seen a huge turnover for apps.  AI is really powerful, so
why are we not seeing replacements for the well-known commercial apps?  Before
AI came out, they were crafted, and the AI hasn't changed that essential aspect
of making a product.  If you want to craft the product you are making with AI,
which involves looking at how each aspect works together, and you want to do so
as a solo software developer, you have to use the AI with lots of additional
approaches and strategies.


AMBIGUITIES, GAPS, AND CHANGES

During the process of making software, there are often times when the planning
cannot account for everything or the goal had ambiguous implications.  The AI
should recognize that this is an issue for all kinds of projects that or the
person often thinks that the plan has something clear but realizes it is
actually somewhat hazy when coming across the issue.

Especially at the product requirements documentation, the AI can also look at
areas that need completion.  What topics are needed before some of the mentioned
ones? When the product requirements document is being generated, the AI should
be asked to look for any contingent aspects that haven't been accounted for.

When the AI is given statements about how the project is to be approached
philosophically, it can suggest features.

Prompt text: "Add features that you think would help or fill in gaps, in keeping
with our philosophy for the project."



ASK THE AI HOW TO DO THINGS (TELLING IT THAT EACH QUESTION IMPLIES THAT IT
SHOULD FILL IN ANY REQUIRED FEATURES IN THAT ARE NOT YET THERE).


The questions asked of the AI, (e.g. "How do I get NCT Framework to make a three
column tree view for browsing the contents of a JSON file?") will then give the
AI the opportunity to find out 1) whether the software can do it already  2)
what needs to be made to address the aspects that cannot yet be accomplished  3)
What, if anything, should be changed when one of these aspects is implemented to
ensure that the whole is integrated and reflects the philosophy of the project.

Prompt: "I am going to ask questions of you, as if I am a user of the end
software, that reveal what aspects of the software need to be made. The
questions asked of you will then give you opportunity to find out 1) whether the
software can do it already 2) what needs to be made to address the aspects that
cannot yet be accomplished  3) What, if anything, should be changed when one of
these aspects is implemented, to ensure that the whole is integrated and
reflects the goals and philosophy of the project."

Prompt: "Here is a basic situation for our product [e.g. sample code for a to-do
list app for a software development framework]. I am going to ask you questions,
as if I am a user of the end software [e.g. the dev framework ], that reveal
what aspects of the software need to be made if our software cannot accommodate
the situation. The questions asked of you will then give you opportunity to find
out 1) whether the software can do it already 2) what needs to be made to
address the aspects that cannot yet be accomplished  3) What, if anything,
should be changed when one of these aspects is implemented, to ensure that the
whole is integrated and reflects the goals and philosophy of the project."

For a software development framework this is especially important because there
are all kinds of scenarios that have to be anticipated and this would require a
team usually to work on it.  But if you have the AI start with a basic app and
then ask it how it would add certain features to the app, it can then
simultaneously come up with the features that are needed, fit them into the
existing structure of the project.


REFERENCES FOR HAVING THE APPROACH A PRODUCT

If you want the AI to approach the product development a certain way, it is best
to give it lots of references and a background on the goals and philosophy of
the project. This would mean lots of instances on how to handle different
circumstances.  If it were an Apple type of consumer app you wanted it to make,
you would give it lots of examples of how the software is to be put together,
how to handle different situations for the end user, how the GUI is constructed.

For apps made with Bimodal Control Theory, the AI can be given a document that
explains the theory, featuring explanations and guides for how to use existing
pieces of software that implement it (NPSurfer, the vector-drawing demo, the
tablet computer demo).  You explain to it in this document that every control
makes use of key-clicks, which comes from the Bimodal Control Theory.


PROBLEM RESOLUTION STRATEGIES




"TASK MASTER" AND THE USE OF MERMAIDJS DIAGRAMS FOR OVERVIEW

Potential Integration and Complementary Use

Task Master drives the “how” (execution), while the MermaidJS diagram strategy
clarifies the “what” (structure). In an AI programming environment, this
combination can streamline development, enhance collaboration, and provide AI
tools with the context needed to analyze or contribute to the project
effectively.



AN APP THAT ASSISTS IN PRODUCT DEVELOPMENT AND PROJECT MANAGEMENT TOGETHER

The purpose of the app is to assist in the combined issues of product
development and product management. Tasks and subtasks can be browsed in a
three-column view. Diagrams are viewable.



LOGIC TAGS GENERATED BY THE AI FOR THE PROGRAM

<actionBlock> <condition name="" states=""> <sequence> </sequence>
<action></action>

</actionBlock> <logic></logic>



INTERACTIVE FORMS FOR AI (.iformai)

For the AI. Provides the form configuration for intermediate step, such as
collecting information.

- includes gui controls - everything editable and deletable by the user for
responding. - user can add questions to the form and answer them such as when
the form's goal is to assemble goals or requirements for a project. - allows for
asking questions to the AI inline. - user can send interactive form back to AI
with revision notes for form updates before submitting it.

- form has different aspects defined for use by the AI: "preform" which is the
example outline, "allowable" which is the types of controls permitted,
"objective" which is the purpose of the form. "config" is the max or minimum
count of questions. "arrangement" is the layout of the form, whether it is a
slideshow, tab, etc.

- form tree: can end up asking just a single question to begin with, which then
branches out into larger form of questions.  or can just ask a single question
and return with multiple choice.  
