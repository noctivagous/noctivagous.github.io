
Moving Beyond Apple's iOS to The Future

noctivagous.github.io


For tablet computers, iOS (iPadOS) has a major drawback: the simplest tasks are easy to achieve, but anything slightly complicated has to be specifically learned because it is invisible or offscreen.  The "user friendly" description only applies to the fundamentals of launching an app, zooming inside a document, and scrolling. After that, the system software features that control the device are tacked on, one after another.  The consequence is that a user can easily activate one of iOS's many side features on accident (often without ever having been introduced to them), and this is a poor state of UI design.  But actually, iOS is an undeveloped and prototype-level collection of user interface conventions, incomplete except for consumption of media.  Because it strives towards minimalism and commercial accessibility, it is nubby, dumbed down, and evokes feelings of interacting with playground equipment.  For controlling software, Apple insists on depriving the user of complementary physical controls like knobs, sliders, and buttons, believing that those were the problem in the first place.

Steve Jobs just knew what mattered at the time-- to put together a tablet UI that worked for the hands and didn't require a stylus, and to accomplish that he brought in the multitouch technologies that had just been introduced by researchers.  The minimalist ideology he imposed was secondary and isn't the foundation of the iPad's success.  In chronological order, Apple developed first a large, tablet-sized multitouch screen and then shrank that to make the iPhone. Then, a few years later Apple released a multitouch interface in its original size as the iPad.  Embedding a foolproof, kiosk type of user interface inside a mobile tablet can sell a lot of devices because anyone from the public can use what iOS provides.  But it won't ever achieve what a PC (desktop computer) can, which is the current problem, that no one has manufactured a truly finished touchscreen product.  Software will never be programmed on the iPad itself without the use of desktop computer accessories that make it behave like a PC.  Currently Apple's software development system is not available for iPads.

It's the interface that has caused this. Making a device blunt in its simplicity comes at a steep cost, because eventually the apps that people want to make and use are more complex, and software developers run into the absence of a systematic UI scheme matching their needs so as to mirror PC functionality.  The root problem is that iOS is ideologically narrow, deeply determined to package bare interactive essentials as the environment for mobile life.  It doesn’t possess any conceptual frameworks to accommodate complex usage of a tablet computer.  Necessarily there are PC trackpads and keyboard sold for these tablets, to turn them into PCs with screen cursors.  Whenever the tablet software is intended for serious usage this basically has to happen.  No cursor would be necessary if iOS carried a complete set of UI principles, but achieving that will involve breaking out of a minimalist ideology that shuns accessory physical controls on the device itself.

The theory of the PC was developed over a period of years by Douglas Engelbart and later Xerox PARC, so a tablet's return to the desktop computer's conventions isn't a surprise when nothing proportionate has been developed for a multitouch device.  The theory behind iOS isn't much beyond a set of raw conveniences to accommodate the multitouch “gestures” unveiled by researchers in the 2000s, serving to launch apps and press buttons inside iOS apps.  Apple's packaging of that carried a minimalist ideology, which effectively halted the development of touchscreen UI.  For example, iOS is supposedly intuitive but it's unknown to the user how to open a file inside a specific application or how to manage the filesystem in a sophisticated way because the files are no longer the focus, just launching apps, for commercialized simplicity's sake.  The objective of making everything user friendly the point of being stupid and foolproof has resulted in an uncomfortable set of devices for several years now that can scroll, zoom, activate buttons but which severely limit engagement with the mobile computer's broad capabilities.

What iOS is really about is a false belief in a zero user-orientation period.  That is, the user should require so little time to be acquainted with iOS.  But "easy to use" (user friendly) shouldn't be equated with "no effort needed to understand it at all," which would be the domain of preschool childrens' toys.  For computing, that's not a good goal in the end because of the problems it creates, that no one can do anything with it who develops professional software.  Rather, there just isn't enough there to get complex tasks done on par with desktop machines.  There has to be a more complex scheme set up for the future.  It shouldn't aim for being "dead simple" just because profits are most valued by some out there.


Touch Operation Buttons

There are a lot of avenues to go down, and here is one.  An object sits there on the tablet touchscreen, but what does touching it mean?  In real life it moves according to physics.  On a touchscreen computer, the initial answer is vague, but generally "open" or "activate".  This is an operation and that is what differentiates the real world physics from the conventions of a touch on a tablet device.   On iOS, a single touch is restricted to one operation: "activate".  Beyond that things get complicated to make the touch do anything else.

However, if several physical operation buttons were placed to the left the screen in a column, they would be able to specify what a touch means when it occurs, and this will broaden the tablet's user interface a great deal.  Closing a window would not involve touching a close button but instead momentarily holding down the "Close" touch operation button on the left edge of the screen and then touching any window(s), panel(s), or object(s) that need to be closed.  When a certain touch operation button is held down, e.g. an "inspect" button, whatever object the finger touches will be affected accordingly (it will be inspected, it will be closed, etc.).  

To summarize: a touch operation button momentarily specifies what a touch/gesture means just before the touch occurs and then the physical button is released by the user.  The speed and straightforwardness of the interface improves: five objects can be removed from a page quickly by holding down a "remove" operation button, then tapping each of the five objects, then releasing the "remove" touch button.   This kind of setup will allow the user to control software with a wide range of touch operations instead of searching for operations inside the interface.  In addition, the touch operation buttons can have their operations changed  if the physical buttons carry individual LCD screens.  

Thus, what is convoluted to achieve today on a tablet will not be and what is out of reach for the average user who does not memorize specific features will become straightforward when physical buttons are placed onto the device.  The key benefit when there are several touch operation buttons sitting to the left of the screen is that they can accommodate a wide range of touch purposes immediately, and this is a core feature of a new tablet device framework for user interaction.  A “generate” touch operation button will make the touch generate an object at the position of contact in an empty space (or it will generate a child object for the object that was touched).  To illustrate: five objects can be generated by holding down the "generate" touch operation button and then tapping the desired locations of the five objects.  For power users, some of these operation buttons can be made programmable.  Importantly, in this scheme the user will always know how to accomplish tasks because the touch operation keys will be general in their function names, covering a broad range of activities. "Close" can apply to exiting or closing something, usable for many objects on screen.  "Inspect" can apply to a variety of objects on screen as well.   Any inspection popover opened with "Inspect" can be closed with "Close", demonstrating that the user can know how to do many tasks without specific instructions.  The current procedure of extracting these functions (making close buttons appear or digging into the user interface to find the desired function) does not have to take place.



It is also possible that something is deleted not through the use of a "delete" modifier next to a "close" modifier but instead tapping an object twice with "close," with it darkening as a warning that it will be deleted will be the way to delete something.  Other modifier keys include "route" such as for opening a file in a specific application, because "route" is a very generic keyword that can apply to many situations.

We could mention many buttons, such as "group" touch modifier button.  But often operations are in opposition to each other. Opposing touch modifiers (group --- ungroup,  add --- subtract,  generate --- remove) on opposite sides.

"Toggle" touch operation button or the duo of "Off" and "On".
  


NOCTIVAGOUS.GITHUB.IO