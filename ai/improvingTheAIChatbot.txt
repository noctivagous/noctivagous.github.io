
Improving the AI Chatbot

INTERMEDIATE RESPONSES I: CLARIFICATION QUESTIONS

Sometimes the AI chatbot user doesn't anticipate the different directions the
response could go, or the variations in contents it could have, especially when
asking about a topic that is unfamiliar. This is definitely true for programming
projects, where a seemingly simple query to the AI can entail many possible
approaches for the code. This points to the need for an intermediate stage of
response, which is a clarification menu that shows up before the output is
generated (a feature that is turned off if desired).  The AI would use it only
when necessary.  Initially I would try mapping the ASDF keys in reverse order
[FDSA] to a 4-option menu, so that the user can proceed rapidly just by pressing
a key and not get hung up by intermediate clarification questions. "For this
code, for the [topic] do you want F) D) S) or A) ?". This menu would only appear
when the AI determined it is beneficial.  It would not be limited to code
situations but rather is present at any time.  (The user could also insert
shorthand into the prompt for requesting varying degrees of clarification
menus.) Pressing RETURN will just move past the menu. When relevant, for more
involved matters, the clarification menu would carry multiple layers of
questions (a tree of questions) when the AI knows that a satisfactory response
likely requires that the user give multiple specification answers in advance.
The key issue is that the user does not always know the issues implied by a
query, and so many pages of text are generated in the chat session as the user
tries to refine the output, whereas often the AI could have asked clarifying
questions.

For the shorthand, the user can dictate inside the prompt which issues of
specifications it wants the AI to ask questions about.  For a code project, the
user might type [specq: programming library, variable naming. depth:4,
quantity:4] and then get questions from the AI in these areas in a quantity of
questioning level 4/10 and tree depth of 4/10. Thus the user can be proactive
about the output from the AI.

What this means is that for any prompt there will potentially be two API calls:
there will be two when the AI has determined that clarification is needed for
the prompt while there will be just one, the response as usual, if it doesn't
need clarification and no menu will appear.  The AI will be given a
clarification frequency value ranging from 0-10, with 0 being never ask for
clarification and 10 being always ask for clarification.  The default is 5 which
means ask for clarification whenever the prompt from the user is vague or there
is information the user did not anticipate.  These instructions about the
clarification menu will reside in the system prompt.  When the client receives a
response with 	</aichat:clarification-response> it will render the menu and
options accordingly and also handle key and mouse down events for the menu
items.

The user should be able to make the clarification response longer or take it in
different directions, because at the time the clarification response appears the
user will know other implications in the project or query.

There is another case where, if the user makes an observation and sends it to
the AI, such as the user is correcting the AI's response, the AI should, just
like a person, make a statement that validates the comment but then also show a
clarification menu for what to do next.  For example: "What I see in the code is
that lines 100-123 in api.js are in the middle of [computer task] and it isn't
the place to detect [some kind of state in the program]. "  The user might not
have an answer as to what to do right there but the AI can express agreement and
then provide some options as what to do next. This situation also points to the
need to have subqueries, or subprompts that are present only within the
clarification menu so that the user can refine the plans of the AI before the
next step.  Chat sequence steps will then have hierarchy. The overall one is the
one today and it is flat.  But in this example there is a possible nested layer.

Alt+LeftClick/Opt+LeftClick on a clarification option will open it in a tab of
the current session, which is in keeping with our section below, "BRANCHING OFF
AND SIDE QUERIES." On mobile and desktop there will be a small button affixed to
the side of the button that is a symbole for "branch off" that puts it in a tab.

We also want dropdown menus to happen, such as "Programming Language : [Python]"
which will be represented by
<nct:configoption-dropdownmenu><option>Python</option>
<option>JavaScript</option></nct:configoption>.


INTERMEDIATE RESPONSES II. MANUALLY-TURNED ON AND OFF: "SPECIFICATION
QUESTIONNAIRES" (WITH SWITCH NEAR PROMPT TEXT BOX)

The user may choose to deliberately write initial requests or questions and
then, when the mode "Specification Questionnaires" is turned on with a checkbox,
the AI will generate specification information collection forms or
questionnaires, of a length determined by the user.  This allows the user to
have the AI collect all of the implications it finds for a project.  It has
multiple uses.  There are a lot of times when the user knows that there are
aspects of the projects that will come up that the AI will know about that the
user might not consider.  The user might make some objective with slightly
ambiguous implications that require distinctions, issues which the AI will often
recognize and bring up.

Additionally, there is the case with these questionnaires where the AI will
gather feedback from the first screen, process it, and then have new questions
to ask based on that information. So then it will generate a new questionnaire
screen in place to get more information. In this way it will have a much better
impact on the final result than the user pushing back and forth with AI chat
responses like right now.


An example:

Q: Write a sort algorithm R: What type of algorithm, language, and instructions
do you want? -- Algorithm Type - Specify which sorting algorithm, like bubble
sort or quicksort. -- Programming Language - Choose a language, such as Python,
Java, or C++. -- Other Details - Provide any additional preferences, like
complexity or examples.

Whether it is automatic clarification questions that appear or specification
questionnaires, in both cases a breadcrumb or history can be stored, including
in the form of sequential tabs from left to right, so that the user can explore
different aspects of the same topic.


INTERMEDIATE RESPONSES IIb. SAMPLE SELECTIONS MENUS

For tasks such as translation, the output can have different variations
possible.  The user can ask the AI to give a list of sample output, allowing the
user to choose what style, for example, will be applied. This can apply to color
schemes, design layouts.  The AI can give previews of different solutions it
will make for a code problem.


INTERMEDIATE RESPONSES III: MODIFIABLE OUTLINE AND PLANS

The AI provides an interactive form, with textboxes underneath each section and
editable outline points, that indicates what it plans to do next.  The user then
gets a chance to make changes.



BRANCHING OFF AND SIDE QUERIES

One issue is that there are times when the user wants to know more about a
subject found within the generated text but not necessarily interrupting the
stream with a query about it (which will also affect later responses).  There
can be a mode provided in the AI chatbot where sentences and paragraphs can be
clicked on, which opens up accessory or marginal tabs for exploring topics in
the original generated response.  The output will be associated with the main
chat sequence but sit to the side or behind it.  In other words, the user should
be able to branch off from the initial chat session and return to it.  The AI
chatbot can certainly have other tools too, like built-in dictionaries and the
ability to add categorization tags to chat sessions.

From a menu of options (e.g. clarification menu), once a user selections one of
them, the result is a new tab.  The user can then go back to the first menu and
select a different option and a tab will open next to the previously generated
tab.  Each time a question menu or interactive form shows up, it will occupy its
own tab. This allows the user to go traverse the AI output, backwards and
forwards.

"Send to Scrapbook" button on any chat response allows sending the contents of
that specific response to the client's scrapbook.


HYPERTEXT FEATURES

When processing text, the AI might have another agent activated that adds
metadata features to the text.  These might include definitions for certain
terms, especially for translated materials, and contextual information.  The AI
can generate hypertext features, assisting in the use of text.

When the question mark cursor is activated during hypertext mode, clicking on a
term will open up a bubble explaining it, querying the AI for the popover.


INTERACTIVE FORMS

The AI chatbot could also respond to the user's code (programming) query with an
interactive form where it has generated an outline of what it plans to do for
the project.  Underneath each outline point are text boxes where the user can
change or refine the AI's plans.  At the end of the form, the user can then
choose to either receive an updated interactive form or it can have the AI
proceed.  In another variation, the AI would include questions inside the
outline and the next step would be the interactive form that the user reviews.

This also shows that the interaction with the chatbot that the user has with the
AI doesn't have be at the end of response and separated from it but can be
interacting with controls and textfields that the AI embedded inside it.

The AI isn't always going to generate what you want. The AI chatbots could
certainly implement intermediate steps where the user can get an overview of its
plans, especially for programming projects, and then refine them iteratively
before anything is generated.

INTERACTIVE FORMS II

The AI is given a list of tags that it can send to the client to represent
controls (for example,  <aichat:textfield>) and the AI can send these inside a
type of chat response that it uses as an intermediate step, to gather
information before generating the final output.  These include 1D and 2D sliders
(<aichat:slider>), multiple choice lists, color pickers, checkboxes, etc.
Replying to the AI after it embeds these in its response is then about
interacting with the chat response, not typing in the followup chat textbox as
is the case for everything now.  At the bottom of this interactive chat
response, the AI will have generated different submit buttons according to the
situation.  The AI can also, after receiving a submission, update the contents
of this interactive form in the place where it sits instead of generating a new
one underneath.  (It would send a command to the client like
<aichat:updateinplace>). What this implies for chatbots is that the the AI can
generate different types of chat responses (data types), some of which include
embedded interactive controls that allow it to tailor its output extensively in
tandem with the user. It can get a lot of information before carrying out the
request.  In a more advanced version, the interactive form that it generates
will be running code so that the AI can collect information from interactive
diagrams (https://github.com/mermaid-js/) that the user modified.

The interactive forms can provide just a general outline of the AI's proposed
plans or they can include sample details of how the AI intends to execute the
task, such as sample text or code.  In a translation effort, for example, the AI
can generate multiple options for the style of translation.

Other tags include <aichat-control:curveeditor>, <aichat-control:sequencer1d
cols="8"> for a drum sequencer, <aichat-control:sequencer2d rows="5" cols="8">
for a musical note sequencer. In the software, the user would be able to add
custom control tags for custom use.


INTERACTIVE FORMS III

Every response from the AI chatbot could make use of an interactive form, which
would reduce the page length of any chat session by allowing modification and
feedback in place by the user, who decides whether to update the response in
place by using the corresponding submit button at the bottom.  The alternative
is that the user just uses the existing text box to respond as usual.  This
means that every response would make use of some of the control tags sent to the
client (e.g. <aichat-layout:flexboxcontainer>) so that the AI produces complex
layouts with interactive controls.  The issue is that the AI usually can only do
one thing well at a time, so it might require a second request to reformat the
response within the tags.


INTERACTIVE FORM TAGS REFERENCE

<!-- gui controls --> <aichat-gui-control:button> <aichat-gui-control:textfield
dataformat="plaintext"> // richtext <aichat-gui-control:numberfield" > //
integer, float <aichat-gui-control:checkbox> <aichat-gui-control:radiogroup>
<aichat-gui-control:radiobutton> <aichat-gui-control:slider-1d>
<aichat-gui-control:slider-2d> <aichat-gui-control:slider-circular>
<aichat-gui-control:colorpicker> <aichat-gui-control:sequencer-1d>
<aichat-gui-control:sequencer-2d>

<!-- layout --> <aichat-layout:flexboxcontainer> <aichat-layout:tabview>
<aichat-layout:tabview-tab>


<!-- external data --> <ai-chat-document-viewer type="pdf"> // for fetched
documents <ai-chat-iframe url="">

The client will construct the various interactive components on the fly based on
the provided tags, amounting to mini-applets in any given response.


CUSTOM PARAMETERS (AND ACCOMPANYING GUI CONTROLS) FOR AI CHATBOT

The clarification frequency is a parameter controlled by a slider in our AI
Chatbot Lab, for determining how often the AI interjects with an intermediate
clarification menu for vague questions or cases where there is information that
could assist forming the answer, including when the user could not have
anticipated that the question would be highly beneficial.  The user should be
able to add custom parameters and matching GUI controls like this of his own
that affect the output, such as tone.  There can also be presets from a library.

In settings, these can be defined by the user, represented by this tag:
<aichatclient:customparam min="" max"" effect="">



DESIGNED CHATBOT OUTPUT

The first call to the LLM only generates design layout placeholders in a
wireframe containing a sentence in each container about what it should contain.
Then the wireframe contents of this response are sent with the initial prompt to
the AI again to fill in the containers with text.


THE AI CHAT APP HAS DIFFERENT LAYOUT PRESETS FOR DIFFERENT USES

Quick interaction for just getting an answer.



THE AI CLIENT RECEIVES DIFF COMMANDS INSTEAD OF REGENERATING ENTIRE FILES

A problem some software developers mention with the AI chatbots is that while
working with them they will write over their good code or remove sections that
were working.  This has to do with the AI having to do too much in one query
because it regenerates the entire code file from scratch rather than making
changes to specific sections.  When it regenerates the code, it can't handle all
of the different aspects from the beginning and omits things that were there.  I
think there is potential in having the the client receive a list of diff
commands, with insertions and deletions being generated by the AI instead of the
entire file.  To do this, it might be beneficial to generate unique IDs for each
line number.


PLANNING AND PROJECT TRACKING WINDOW

This represents a broader usage of the LLM within an app beyond simple back and
forth chatting, where the app integrates other aspects. Persistent planning and
outline window or tab for projects, optional. Updated as project progresses
while also editable. A maintained tracking window.

A similar window for writing code is one, driven by an AI agent, that maintains
a navigable diagram (MermaidJS) of the project, where clicking on an element
will go to that section in the code.


CLIENT-SIDE COMPONENTS

Webview and code editing in two different tabs for updating the result.


PHILOSOPHICAL COMMENTS

People are generally focused first on what the AI can do for them directly, how
they can gain from it.  The software developers think this way, and from this
mindset comes their treating AI as a way to automate or achieve without effort.
They make products around these limited goals.  This is a dead end, because when
software is built from this mindset, the AI can never play the role as a tool,
only carry out tasks as a servant.  Those are two different states. A better
starting point for the software developer is to identify what the gaps in
capability and knowledge are between the person trying to use AI and what the AI
itself can do and has available, for any task.  Let's say the person wants to
write code for a project.  The AI knows all the ways the person's code can be
written and how his or her project can be put together, as well as the
implications of any given move he makes, at any given step.  It's extremely
capable in this respect.  The person doesn't have that range of knowledge, so
why is he dictating to the AI step by step, as if he the one who knows?  An LLM
has inconceivable amounts information available on any given topic, and if you
ask it a question it has to select  from its information and summarize.  This
means the software developer didn't give it a chance to tell him what it knows
before he gave it instructions and he goes down a narrow path of reacting to its
responses (ChatGPT today) and struggling to get what he wants.  Then he can't
truly make use of the AI as a tool, because he isn't acknowledging what it has
versus himself.   In many cases, the available instructions should be generated
by the AI itself.   You react to those and thereby continuously inform the AI
based on what it tells you.   This is genuine interaction with the AI and it
will produce superior results.  You have allowed the AI to play its role as the
thing that knows, and you equip it with preliminary information at any given
step from options it generated from its knowledge.  The AI helps you instruct it
by giving you relevant instructions to give it for the current situation or step
in the process.  The AI has to be allowed to interject under certain conditions,
such as if the person's request is too vague and needs clarification, because it
knows more factual information.   By allowing the AI to interject, and telling
it how to interject and under what conditions, it gets valuable information for
the user's next step based on its vast collection of knowledge.

DISCUSSING THE CODE-GENERATING EDITORS

On the one hand, it is impressive to see code inserted into a project, entire
files generated from AI prompts, and the previously challenging-to-assemble app
running in 10 minutes.  On the other hand, though, these   code-generating
editors are still lacking process for software development, which may seem like
a minor point when they can do so much.  Process matters: when the feature
omissions are there to accommodate what software developers do outside of
writing the actual code, an app like "Cursor" becomes about constantly catering
to the AI prompt text box and chasing down the quirks and idiomatic behavior of
the automated code IDEs.  These IDEs do not make, as the center or anchor for
the software developer, a prominent list of the programmer's todo items or the
project outline.  This would exist inside a dedicated pane (expandable to a
window), where the code IDE checks off list items that have been accomplished
and has expandable aspects of tasks to be done.  As long as the contents of this
outline in this pane is always editable by the user-- since the user may need to
change aspects of the project midway-- it will assist in the unpredictable
process that is making a software project from scratch.  Also, as long as the
outline pane is synced with the contents project at all times, the AI agents
will play the right kind of role.  Software development isn't just about writing
code but also referring back to the big picture, something the IDEs have never
truly assisted with.   It becomes even more important now that there is AI
becoming the focus of the job.  This outline pane can have multiple tabs for
multiple parts of the project (appearance, data models, etc.).  It should be
more than just a to-do list editor and can embed other capabilities of AI to
interact with the software developer, such as displaying menus of options for
paths the software developer can go down, interactive menu sequences,  a sidebar
for pointing out considerations and giving suggestions for macro and micro
issues of implementation.  In aspects besides an outline pane, such as working
with the code files, there are areas for process improvement as well.  These
IDEs aren't providing or emphasizing action buttons that automatically document
the code or keep them in sync with a documentation file.  Supposedly that is up
to the software developer but all good practices like this should be included as
options in a development environment and made convenient too.


UTILIZING PRELIMINARY DIALOGUES / INTERMEDIATE MENUS FOR GUIDING CODE GENERATION
AND EDITING FOR DIFFERENT LEVELS OF THE PROJECT

When a code editor assists a person in generating code, what is left out of
these code-generation editors is an acknowledgment that there are different
approaches to code and corresponding guidelines that the AI agents can follow. 
This is opposed to the current situation where it just write code for a request.
 Should the AI, when generating the code and analyzing the project, move it
towards procedural, declarative, or some other programming philosophy?

As is said above, what is important is that the user be provided options
generated by the AI to address these different issues.  It's true that the
software developer knows of these aspects of a project when they're mentioned,
but the AI can offer a a menu selection.  These preliminary dialogues with the
user can help it can establish its overarching approach to the software project,
in a way where the software developer is providing input at the same time, not
just answering questionnaires.  How should the AI generally style the appearance
of the app?  Does the user want to set up some design guidelines?  The AI can
make suggestions for this that the software developer wouldn't have brought up.

Approaches for the large scale overview might involve the file folder
organization, how the AI code agent will tend towards dividing code into files.
By what principle does it suggest (or decide to make) a new file for a section
of code?  Is it monitoring the project to tell when it is time to suggest to the
user (or do it on its own) that a certain collection of code should be
reorganized, divided into other files? This is currently entirely up to the
software developer.

Approaches for the medium scale might involve the code files and how they are
written and organized to relate to one another.  Approaches for the micro scale
would include how a line of code is written, how variables are declared, how .

Though the user could hypothetically do all of these by himself or herself, the
reality is that they should all be managed by the AI.






