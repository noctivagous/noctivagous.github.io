The next generation of AI chatbot front-end should be our focus. To begin with, we can recognize the concept of putting the AI chatbot into modes, such as step-by-step walkthrough (a step–result–interpret loop), with the UI of the app changing (there being different window layouts) to match each mode.  A step-by-step walkthrough mode applies to troubleshooting a computer, an appliance, etc., and would present tasks on slides, one at a time, with a large text box to the side for the user to insert results.  This is unlike anything else because it will of course be generating the tasks in response to the results entered by the user for each step.  A complex 3D modeling software package can be learned this way, or anything else where questions come up because of confusion. Another mode would be a teaching mode for course instruction, and the GUI would change to accommodate that.  Other modes include one that gathers information in the form of questionnaires before taking action, such as to set up a software development project or another type of project that has many placeholder aspects that have to be filled in.    Modes could be programmed and supplied by users in the form of files that have a sequence and flow descriptor data format, which describe which GUI to use for each mode.

To simulate a walkthrough mode in today's AI chabot, give it the following prompt: "I want to ____.  Let's do this one step at a time and I give you results. A step–result–interpret loop."

If this AI app is running on the desktop, it can be given an AI agent with read-only program execution privileges that allow it to execute apps and open up windows, which would allow it to make an on-the-fly orientation for any user to a system like Linux or an app that is unfamiliar.
