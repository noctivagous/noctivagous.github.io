

THE AI GENERATES SITUATIONAL PROMPT DETAILS (PARAMETERS) 
FOR THE SITUATION FOR THE USER TO CONFIGURE IN MENUS.

--> A PRELIMINARY MENU IS GENERATED BY AI BEFORE FULFILLING A REQUEST 
THAT ALLOWS THE AI TO GUIDE THE USER BEFORE THE AI UNDERTAKES WORK.

--> IF IT IS AN IMAGE BEING GENERATED, THE AI COMES UP WITH
ALL OF THE DETAILS OF THE IMAGE'S SCENE THAT THE USER WOULD WANT TO
SPECIFY AND THEN FORMATS IT INSIDE A PRELIMINARY MENU OR FORM.

-------> AFTER THE USER MAKES SELECTIONS FROM THE PRELIMINARY MENU AND
SUBMITS IT, THE AI GENERATES OUTPUT. THROUGH THIS IT GENERATES 
MORE SATISFACTORY, TAILORED OUTPUT FROM THE START.


SUMMARY:
--> In Many Cases, After The User Submits The Initial Prompt,
The AI Should Generate Details for The Situation
For The User to Fill Out or Configure Before It Generates Output. 


---


Often it is the case that too much labor is required of the user to put together 
a complete and detailed text prompt to get a satisfactory result from AI on
the first submission.  The usual sequence involves the user repeatedly reacting to what the 
AI generated in order to correct and revise it, which is a labor-intensive 
and tiresome process.  This happens because the user hardly ever
provides comprehensive details from the start and lets the AI operate off of
its assumptions and guesses about what the user wants.  But there is no way to accurately
know what the AI will assume every time.  This is a key reason that usage of AI in
AI chatbots falls short of expectations.  The AI should be involved in proactively 
suggesting and collecting specifications for the situation in preliminary menus. 
The user may not anticipate everything that a situation entails.

Take for example the generation of an image.  I said to the AI, "Make a playing
card with the image of a joker on it."  I found out, after getting the result,
that what the AI had in its assumptions was far from what I was expecting 
(or perhaps what other people would expect too). It made an image of a 
joker featuring an actor from a recent movie in a photorealistic style inside 
a life-sized playing card frame.  It turns out that I was going to have 
to tell it a significant number of specifying details upfront: 
	- what illustration style (photorealistic). 
	- what the joker should look like. 
	- what the joker should be doing. 
	- The position of the joker.
	- The clothing.
	- etc.

That's too much to ask of the user for every AI prompt. For routine usage 
of AI, such a wide-ranging list of situational parameters takes too much work to 
come up with on the spot.  It is work that the AI should generate.  It can
provide this as the first response, in a preliminary step before the actual generation
of the image occurs.

To summarize, the chat exchange should really be the AI coming up with these kinds of
situational parameters first and then asking them in a preliminary menu (that features GUI
form controls like dropdown menus, checkboxes, etc.).

Example:

--- User: Make a playing card with the image of a joker on it.

--- AI: You said you want a joker on a playing card.

	- What illustration style? [dropdown menu] 
	- What kind of joker? [dropdown menu] Other: [Textfield]
	- What position should the joker be in? [dropdown menu] Other: [Textfield]. 
	- Looking in what direction?  [dropdown menu]
	- What clothing style? [dropdown menu]
	- This is a playing card that is on a table or life-sized?
	More specifying information: [Textfield].

-------

This is a set of details that the user might not fully anticipate, will have
a hard time listing off just for routine usage of AI.  The user will 
only find out many of them are relevant after the image has been generated.  
Then the user will have to respond with revising instructions to generate 
new images.

If this is image is being generated in a node flow diagram scripting environment, 
then it would be the AI generating and displaying a preliminary menu that generates 
the nodes and their default parameters.

It should be that the AI is taking care of the intermediate work of providing
a menu of parameters for the context.



--

A software application is useful proportionate to the amount of control the user
has over the output, with an application like Excel demonstrating high levels of
user control (data entry cells, charts generated).  The usability of those 
controls and a smooth arrangement of them is what makes quality software.  
When, however, software produces flashy, advanced output but the user's ability to 
control the output is limited, e.g. AI-generated images produced just from text prompts, 
then this is largely a novelty or an uncomfortable and unfinished situation.  
The effort required to turn the output into something of value is too high.  
AI generation of images can be put into other software contexts where user control is 
higher, such as node flow diagram scripting environments.  

During an onstage interview, an OpenAI executive called ChatGPT "the world's greatest 
parlor trick."  People everywhere have managed since the time he said that to make 
ChatGPT work for them in spite of the undeveloped state of the front-end.  It is 
still the case that ChatGPT produces a novelty effect because the front-end software 
is incomplete. The amount of control the user can have over the output is much less 
than it should be (e.g. there is no control over output length, it is hard to steer 
responses, etc.).  Addressing this gap requires more than just the addition of gui
controls.  The AI should generate parameters for any situation.  Intermediate menus are 
important for AI chatbot software like ChatGPT to assist the user in making use
of the AI.




