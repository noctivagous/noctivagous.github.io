

The Issue of Converting User Interfaces from Desktop to Touchscreens and VR


The future won't happen just because a device displays objects in 3D or carries a touchscreen.



NOCTIVAGOUS.GITHUB.IO

The iPhone and iPad were truly able to take off when websites began providing mobile versions of their web pages.  The lack of mobile versions of pages was a major obstacle for previous mobile devices' browsing of the web.   VR has stagnated for the same reasons found in those early mobile devices and for it to succeed there have to be 3D versions (or at least what people call "2 1/2 D" versions) provided of the same web page and the same apps, because whenever the personal computing format changes everything has to change along with it, just like how mobile apps are different from desktop ones.  Of course a 3D web concept is vague currently and hasn't been investigated, but at least the current web could be brought out into 3D, something Magic Leap considered with its "detached transforms" (https://github.com/magicleap/detached_explainer), a quick and dirty method any website could adopt today if it wanted.  No matter what is happening in VR, the public will continue to use the web and when there is no real implementation of 3D inside a 3D device, why would a person find an advantage to using it over a regular desktop computer or mobile device for browsing the web?  The web itself has to have a 3D version and that requires that companies adopt the use of 3D tags for alternate versions of the same page.  When there is a VR headset in use there is the opportunity to browse galleries of 3D objects on a web page, but this has gone completely ignored somehow despite being something highly desired in the early days of the web.  The gallery presentation itself would have to be 3D in some way.  It might not take up the entire space of the room but it would provide conventions for being 3D.  Any omission of this indicates a lack of comprehension of what user interfaces have to do when they move across hardware formats, which is transform or be reworked from the ground up.  In VR, a web page would no longer be a page but would take on a diorama format, with many 3D elements and objects sitting on a wide table in front of the VR user, inclusive of text boxes and navigation components like today; opening a new "tab" would be moving a such a table out to the side or outward into the distance in front, to the left, or to the right.

Understandings of user interface are inadequate in companies across all of Silicon Valley and have been for decades, and these understandings have to be upgraded in order for the next generation of personal computing to take hold, whether it is holograms, VR, or mobile.  It is often observed that the iOS and Android tablets are unfinished products; for serious work a trackpad and keyboard have to be attached, which effectively reverts the products back to desktop computer interfaces (in this case laptops) that benefit from some touch conveniences. Too often a person might as well use a normal laptop instead.  Related to this are the problems found the Apple Vision Pro and all other VR technology, that the user interface theory just isn't there for them to produce what people hope for, which is a futuristic world where people get work done inside these devices.

The only truly finished personal computing technology in the last 40 years that lets people get work done of all kinds has been the desktop computer, which was developed by Douglas Engelbart (the person who devised the mouse) and later Xerox PARC which introduced the bitmap (pixel) display.  In the 1980s, this personal computing format was brought to consumers first by Apple with the Macintosh.  It's important to remind everyone though that Apple borrowed from research and technologies that were developed a decade earlier, elsewhere.  In the same way, the multi-touch technologies that make up the iPad were borrowed by Apple from research and packaged into a tablet format on top of the company's own operating system.  But the development time for the iPad was very short and that is also true for VR interfaces.  For that reason, compared to desktop computers, the theory behind the iPad and VR headset is too shallow for accomplishing a wide range of work, being driven too hard by commercial pressures and entertainment.  There is no way that later technologies like VR or multi-touch tablets can succeed on par with desktop computers without revisiting what they lack in terms of user interface theory and what their teams are not providing the user.

There are other business strategy facets to the situation, too.  In the VR industry, for example, the emphasis has been on video games to convince the consumer of the VR technology's value.  But new technologies that are introduced to the public have to be rooted first in what people need in life and work, not what will entertain or amuse them, because entertainment and gadgetry will never branch out to utility, the essence of what technology addresses for society.  Utility is the core of technological progress and adoption, then later these same technologies are used to entertain.  False utility is also a problem, however, which is the domain of gizmos, gadgets, and gimmicks, explained easily by the old informercial products of the past.

Specific to VR's user interface problem is the matter of how both ends of the UI have to be modified to match today's technological world for the tech to make sense: both apps and major websites have to implement support for 3D versions of what they currently show in 2D (such as what Magic Leap had suggested, https://github.com/magicleap/detached_explainer). Interfaces inside the VR headsets have to be essentially in 3D in some way, not flat sheets of paper. These mainstream companies commonly understand UI theory halfway: in 2010, Apple executives, including Steve Jobs, asserted that the iPad would result in the "post-PC era," revealing how little Apple as a company understood about what it was doing, how what it actually had done is incorporate the multi-touch technologies that had been around for a long time and fused them to a tablet.  The public believes Apple is the supreme expert in all things computing but actually their engineers and user interface experts work inside very narrow conventions shaped by a highly profit-oriented corporate environment.  Within that, they work extremely hard, paying close attention to every pixel, but they miss the larger picture all the time and they don't regard Xerox PARC history as more than a set of facts.

The iPhone and iPad stick around unchallenged because generally there are few people who seek out user interface theory in a deep way.  The first tablet computers that appeared in the 2000s presented the consumer with an unmodified desktop computer screen, to give an example, something even less than what Apple provides with the iPad.  Interacting with the tablet software was tied to the use of a stylus, which indeed is inadequate.  Apart from lacking any multi-touch capabilities on the screen, the computer companies did not recognize that the input and interface to the operating system, a tablet screen, had just changed, moving from mouse buttons and a screen cursor to intermittent touches that have no constant anchor like a screen cursor.  That is, no conversion of the overall system user interface or the apps on these devices was undertaken.  As would have to be the case, these products carried little appeal except for niche users.  They ended up providing, for a mobile situation, a more complicated way to use a mouse cursor and PC operating system.

For the same reason that a car’s frame and chassis cannot be directly utilized for the fuselage of an airplane or the hull of a boat, the desktop PC operating system, its cursor, and the software it runs, cannot be left the same for a touchscreen tablet or smartphone device.  This applies to all situations, including VR. When the overall input configuration and form of the hardware changes, everything else in the conventions of apps should react too.  In some cases this is apparent to everyone as part of the technology shift, such as how VR games have to be developed differently than traditional 3D video games, and so everyone makes the change.  In other subjects, it is less obvious and continues to exist in an inadequate state, such as how video game controllers still do not detect how much force has been applied to their buttons, even as players instinctively play as if they do. Each button is like an on-off light switch, including the arrow buttons, and the character on screen will jump to a fixed height, not higher if the button is pressed harder or faster.  If that one aspect of a controller changes, all of the video game companies have to change too, and the games themselves will feel more normal.

On a touchscreen, the finger is swooping in and touching the screen's graphical buttons, not a permanent cursor being pushed around, so all apps have to be written or converted for that.  Adaptation has to occur because the computer user can't easily use touch for desktop apps because the desktop apps’ buttons were made for a screen cursor, which is small, and they are smaller than what is acceptable for the fingers.  Further, when it is a touchscreen device that usually has no keyboard attached, this means that all of the keyboard (hotkey) command shortcuts cannot be activated.  Therefore, computing conventions have to match the form of the hardware, whether it is conversion of what is already used (PC desktop to iOS multi-touch) or something built from the ground up (an altogether new type of tablet PC carrying few UI connections to desktop).  iOS undertook conversion, as this was easy, but what is superior is building from the ground up, starting over for the form factor.  The failure of early tablet PCs can be explained as follows: if it is conversion of desktop that is taking place, what was originally the desktop computer interface has to be restructured for apps that sit on a different-looking machine and they end up looking different; the apps as they are developed have to be rearranged for touches, following a consistent set of rules for bigger buttons.  Under Steve Jobs, Apple went about making that kind of modification for Mac OS, transforming its exterior into a separate touch operating system, iOS, for its own type of apps.  It isn't a deep system-level change, however.  iOS is mostly a set of external software conventions to convert desktop executable conventions to touch-based apps, to make apps easy to touch and use.  They have all the essential insides of the earlier desktop apps, to the extent Jobs originally explained iOS as Mac OS on the phone.

This change of user interface has nothing to do with what software can be permitted for execution by the computer itself or the user.  It has no connection to that, whatsoever.  In addition, not everyone would implement the surrounding touch operating system conventions in such a kiosk-like, minimalistic, and restrictive way like Apple or Google chose.  With all of the differences occurring between mobile tablets and desktop computers, an opportunity arose for Apple and Google to deceive the public, that this change in computing format necessitated a restrictive environment on how apps are loaded and executed on the hardware, coinciding with a simplistic style of ease of use, that the two companies need to vet any app executed from within an online store, prohibit custom or independent app execution by the user, and take a large percentage of sales at the same time.  This part is completely artificial, a point made exceedingly obvious by Microsoft Surface tablets: these tablets run any Windows app the user wishes or downloads from anywhere, and they are touchscreen devices.  Microsoft just never cared much for app store profits and it never bothered to make a mobile, touch mode of Windows for Microsoft Surface devices that provided a framework for mobile-specific apps, so these tablets just run desktop apps and carry the problems just mentioned, that  it will be desktop software being used in an awkward way if the software engineer isn't provided a separate set of conventions for touch software (like iOS and Android).   Microsoft could have done this and provided its Surface tablets a set of Windows OS extensions for mobile, offering a kit for software developers to use, and then left access to regular Windows apps on the side for those who hook up a physical keyboard and trackpad.  In the same way, iPads could be made to run Mac OS apps on the side.  The iPad Pro even has the same processor (M2) as the latest desktop and notebook Macs.  This is why mobile devices could run touchscreen apps just freely as desktop machines run theirs.  The enormous effort to convince the public and the government otherwise is entirely motivated by the desire for profit.

What Apple and Google did was make the kiosk-like grid launch screen and the app store the center of the user experience with these devices and then confine the user to them.  An app store is just an app.  At the center of this is that profits are too high to these companies to do the right thing and open the mobile operating systems back up to normalcy, which means they can be used freely by pro users and software developers to execute anything.  It has reached the point that everyone who uses a phone is an unwitting participant of this high fee that Apple and Google imposes on the touchscreen device software developer.  In recent years, Apple has been trying to push this profit scheme onto the desktop software developers too, actively discouraging users from opening any Mac OS app that was not downloaded and vetted by the Mac OS App Store, with popup warning boxes.  All of this is an attempt to satisfy more greed and amass more control over the computer users and extract more money from the software developers selling the software.

------


People are upset that they can't run iPad apps outside of an app store.  Microsoft Surface tablets don't have this problem, but people don't want to buy them because they use touch input on desktop apps.

Previous to Steve Jobs' iPad, launched in 2010, the fundamental problem with tablet computers was that the apps and the system UI were left the same as desktop. The vital step of converting the user interface from mouse cursor to touches had not been undertaken by anyone, indicating a lack of understanding of UI principles by the industry.  Strangely, after more than a decade Microsoft still has not become aware of this and today ships its Microsoft Surface tablets computers without bundling a separate touch mode for Windows apps (which would have to be matched with a corresponding SDK for software developers so they can make touch-specific presentation).  On this matter Microsoft and it followers lament an isolated and unrelated concern, that its touch-based smartphone did not gain popularity.  This is beside the point.  The entire reason why Microsoft Surface tablets are purchasable today is that Steve Jobs' iPad team carried out the work and converted mouse-based, desktop software conventions to touch.  Why then does Microsoft not offer a touch UI mode for its Windows apps when they are shipped on Microsoft Surface tablets?

It is something this minor that can settle the Android and Apple app store debate: every person who writes application software knows that there nothing about a touch UI that dicates fusing it to an app store, as it is merely a reconfiguration of desktop conventions (e.g. making the buttons bigger for touch, removing the system menu at the top, or deciding not to make use of windows).  It takes expertise to this, which is not to be taken lightly, but the tablet isn't an altogether different computer with significantly different OS internals.

Windows apps on Surface tablets should automatically enter a touch-specific mode, loading separate touch-specific XAML files.  Microsoft can settle the app store debate by releasing a Touch SDK for Surface tablets, adding a touch mode to Windows apps.  The ability to add touch functionality to Windows apps is provided in a small way, but it isn't promoted as a comprehensive target like iOS or Android apps, which would make a coherent multi-touch situation that matches the experience of other mobile operating systems.  For the most part, Microsoft Surface is synonymous with desktop apps activated with touch.


