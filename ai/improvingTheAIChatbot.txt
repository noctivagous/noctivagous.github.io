
Improving the AI Chatbot

INTERMEDIATE RESPONSES I: CLARIFICATION QUESTIONS

Sometimes the AI chatbot user doesn't anticipate the different directions the
response could go, or the variations in contents it could have, especially when
asking about a topic that is unfamiliar. This is definitely true for programming
projects, where a seemingly simple query to the AI can entail many possible
approaches for the code. This points to the need for an intermediate stage of
response, which is a clarification menu that shows up before the output is
generated (a feature that is turned off if desired).  The AI would use it only
when necessary.  Initially I would try mapping the ASDF keys in reverse order
[FDSA] to a 4-option menu, so that the user can proceed rapidly just by pressing
a key and not get hung up by intermediate clarification questions. "For this
code, for the [topic] do you want F) D) S) or A) ?". This menu would only appear
when the AI determined it is beneficial.  It would not be limited to code
situations but rather is present at any time.  (The user could also insert
shorthand into the prompt for requesting varying degrees of clarification
menus.) Pressing RETURN will just move past the menu. When relevant, for more
involved matters, the clarification menu would carry multiple layers of
questions (a tree of questions) when the AI knows that a satisfactory response
likely requires that the user give multiple specification answers in advance.
The key issue is that the user does not always know the issues implied by a
query, and so many pages of text are generated in the chat session as the user
tries to refine the output, whereas often the AI could have asked clarifying
questions.

For the shorthand, the user can dictate inside the prompt which issues of
specifications it wants the AI to ask questions about.  For a code project, the
user might type [specq: programming library, variable naming. depth:4,
quantity:4] and then get questions from the AI in these areas in a quantity of
questioning level 4/10 and tree depth of 4/10. Thus the user can be proactive
about the output from the AI.

What this means is that for any prompt there will potentially be two API calls:
there will be two when the AI has determined that clarification is needed for
the prompt while there will be just one, the response as usual, if it doesn't
need clarification and no menu will appear.  The AI will be given a
clarification frequency value ranging from 0-10, with 0 being never ask for
clarification and 10 being always ask for clarification.  The default is 5 which
means ask for clarification whenever the prompt from the user is vague or there
is information the user did not anticipate.  These instructions about the
clarification menu will reside in the system prompt.  When the client receives a
response with 	</aichat:clarification-response> it
will render the menu and options accordingly and also handle key and mouse down
events for the menu items.

The user should be able to make the clarification response longer or take
it in different directions, because at the time the clarification response appears
the user will know other implications in the project or query.

There is another case where, if the user makes an observation and sends it to
the AI, such as the user is correcting the AI's response, the AI should, just
like a person, make a statement that validates the comment but then also 
show a clarification menu for what to do next.  For example:
"What I see in the code is that lines 100-123 in api.js are in the middle of [computer task]
and it isn't the place to detect [some kind of state in the program].
"  The user might not have an answer as to what to do right there but the 
AI can express agreement and then provide some options as what to do next.
This situation also points to the need to have subqueries, or subprompts that
are present only within the clarification menu so that the user can refine
the plans of the AI before the next step.  Chat sequence steps will then have hierarchy.
The overall one is the one today and it is flat.  But in this example there
is a possible nested layer.

Alt+LeftClick/Opt+LeftClick on a clarification option will open it in a tab of the current session,
which is in keeping with our section below, "BRANCHING OFF AND SIDE QUERIES."
On mobile and desktop there will be a small button affixed to the side of
the button that is a symbole for "branch off" that puts it in a tab.

We also want dropdown menus to happen, such as "Programming Language : [Python]"
which will be represented by <nct:configoption-dropdownmenu><option>Python</option>
<option>JavaScript</option></nct:configoption>.


INTERMEDIATE RESPONSES II. MANUALLY-TURNED ON AND OFF: "SPECIFICATION
QUESTIONNAIRES" (WITH SWITCH NEAR PROMPT TEXT BOX)

The user may choose to deliberately write initial requests or questions and then,
when the mode "Specification Questionnaires" is turned on with a checkbox, the
AI will generate specification information collection forms or questionnaires, 
of a length determined by the user.  This allows the user to have the AI collect
all of the implications it finds for a project.  It has multiple uses.  There are
a lot of times when the user knows that there are aspects of the projects that will
come up that the AI will know about that the user might not consider.  The user
might make some objective with slightly ambiguous implications that require distinctions, 
issues which the AI will often recognize and bring up.

Additionally, there is the case with these questionnaires where the AI will gather feedback
from the first screen, process it, and then have new questions to ask based on that information.
So then it will generate a new questionnaire screen in place to get more information.
In this way it will have a much better impact on the final result than the user pushing
back and forth with AI chat responses like right now.


An example:

Q: Write a sort algorithm R: What type of algorithm, language, and instructions
do you want? -- Algorithm Type - Specify which sorting algorithm, like bubble
sort or quicksort. -- Programming Language - Choose a language, such as Python,
Java, or C++. -- Other Details - Provide any additional preferences, like
complexity or examples.

Whether it is automatic clarification questions that appear or specification
questionnaires, in both cases a breadcrumb or history can be stored, including
in the form of sequential tabs from left to right, so that the user can
explore different aspects of the same topic.


INTERMEDIATE RESPONSES III: MODIFIABLE OUTLINE AND PLANS

The AI provides an interactive form, with textboxes underneath each section and
editable outline points, that indicates what it plans to do next.  The user then
gets a chance to make changes.



BRANCHING OFF AND SIDE QUERIES

One issue is that there are times when the user wants to know more about a
subject found within the generated text but not necessarily interrupting the
stream with a query about it (which will also affect later responses).  There
can be a mode provided in the AI chatbot where sentences and paragraphs can be
clicked on, which opens up accessory or marginal tabs for exploring topics in
the original generated response.  The output will be associated with the main
chat sequence but sit to the side or behind it.  In other words, the user should
be able to branch off from the initial chat session and return to it.  The AI
chatbot can certainly have other tools too, like built-in dictionaries and the
ability to add categorization tags to chat sessions.

"Send to Scrapbook" button on any chat response allows sending the contents
of that specific response to the client's scrapbook.


HYPERTEXT FEATURES

When processing text, the AI might have another agent activated that adds
metadata features to the text.  These might include definitions for certain
terms, especially for translated materials, and contextual information.  The AI
can generate hypertext features, assisting in the use of text.

When the question mark cursor is activated during hypertext mode, clicking on a
term will open up a bubble explaining it, querying the AI for the popover.


INTERACTIVE FORMS

The AI chatbot could also respond to the user's code (programming) query with an
interactive form where it has generated an outline of what it plans to do for
the project.  Underneath each outline point are text boxes where the user can
change or refine the AI's plans.  At the end of the form, the user can then
choose to either receive an updated interactive form or it can have the AI
proceed.  In another variation, the AI would include questions inside the
outline and the next step would be the interactive form that the user reviews.

This also shows that the interaction with the chatbot that the user has with the
AI doesn't have be at the end of response and separated from it but can be
interacting with controls and textfields that the AI embedded inside it.

The AI isn't always going to generate what you want. The AI chatbots could
certainly implement intermediate steps where the user can get an overview of its
plans, especially for programming projects, and then refine them iteratively
before anything is generated.

INTERACTIVE FORMS II

The AI is given a list of tags that it can send to the client to represent
controls (for example,  <aichat:textfield>) and the AI can send these inside a
type of chat response that it uses as an intermediate step, to gather
information before generating the final output.  These include 1D and 2D sliders
(<aichat:slider>), multiple choice lists, color pickers, checkboxes, etc.
Replying to the AI after it embeds these in its response is then about
interacting with the chat response, not typing in the followup chat textbox as
is the case for everything now.  At the bottom of this interactive chat
response, the AI will have generated different submit buttons according to the
situation.  The AI can also, after receiving a submission, update the contents
of this interactive form in the place where it sits instead of generating a new
one underneath.  (It would send a command to the client like
<aichat:updateinplace>). What this implies for chatbots is that the the AI can
generate different types of chat responses (data types), some of which include
embedded interactive controls that allow it to tailor its output extensively in
tandem with the user. It can get a lot of information before carrying out the
request.  In a more advanced version, the interactive form that it generates
will be running code so that the AI can collect information from interactive
diagrams (https://github.com/mermaid-js/) that the user modified.

The interactive forms can provide just a general outline of the AI's proposed
plans or they can include sample details of how the AI intends to execute the
task, such as sample text or code.  In a translation effort, for example, the AI
can generate multiple options for the style of translation.

Other tags include <aichat-control:curveeditor>, <aichat-control:sequencer1d
cols="8"> for a drum sequencer, <aichat-control:sequencer2d rows="5" cols="8">
for a musical note sequencer. In the software, the user would be able to add
custom control tags for custom use.


INTERACTIVE FORMS III

Every response from the AI chatbot could make use of an interactive form, which
would reduce the page length of any chat session by allowing modification and
feedback in place by the user, who decides whether to update the response in
place by using the corresponding submit button at the bottom.  The alternative
is that the user just uses the existing text box to respond as usual.  This
means that every response would make use of some of the control tags sent to the
client (e.g. <aichat-layout:flexboxcontainer>) so that the AI produces complex
layouts with interactive controls.  The issue is that the AI usually can only do
one thing well at a time, so it might require a second request to reformat the
response within the tags.


INTERACTIVE FORM TAGS REFERENCE

<!-- gui controls --> <aichat-gui-control:button> <aichat-gui-control:textfield
dataformat="plaintext"> // richtext <aichat-gui-control:numberfield" > //
integer, float <aichat-gui-control:checkbox> <aichat-gui-control:radiogroup>
<aichat-gui-control:radiobutton> <aichat-gui-control:slider-1d>
<aichat-gui-control:slider-2d> <aichat-gui-control:slider-circular>
<aichat-gui-control:colorpicker> <aichat-gui-control:sequencer-1d>
<aichat-gui-control:sequencer-2d>

<!-- layout --> <aichat-layout:flexboxcontainer> <aichat-layout:tabview>
<aichat-layout:tabview-tab>


<!-- external data --> <ai-chat-document-viewer type="pdf"> // for fetched
documents <ai-chat-iframe url="">

The client will construct the various interactive components on the fly based on
the provided tags, amounting to mini-applets in any given response.


CUSTOM PARAMETERS (AND ACCOMPANYING GUI CONTROLS) FOR AI CHATBOT

The clarification frequency is a parameter controlled by a slider in our AI
Chatbot Lab, for determining how often the AI interjects with an intermediate
clarification menu for vague questions or cases where there is information that
could assist forming the answer, including when the user could not have
anticipated that the question would be highly beneficial.  The user should be
able to add custom parameters and matching GUI controls like this of his own
that affect the output, such as tone.  There can also be presets from a library.

In settings, these can be defined by the user, represented by this tag:
<aichatclient:customparam min="" max"" effect="">



DESIGNED CHATBOT OUTPUT

The first call to the LLM only generates design layout placeholders in a
wireframe containing a sentence in each container about what it should contain.
Then the wireframe contents of this response are sent with the initial prompt to
the AI again to fill in the containers with text.


THE AI CHAT APP HAS DIFFERENT LAYOUT PRESETS FOR DIFFERENT USES

Quick interaction for just getting an answer.



THE AI CLIENT RECEIVES DIFF COMMANDS INSTEAD OF REGENERATING ENTIRE FILES

A problem some software developers mention with the AI chatbots is that while
working with them they will write over their good code or remove sections that
were working.  This has to do with the AI having to do too much in one query
because it regenerates the entire code file from scratch rather than making
changes to specific sections.  When it regenerates the code, it can't handle all
of the different aspects from the beginning and omits things that were there.  I
think there is potential in having the the client receive a list of diff
commands, with insertions and deletions being generated by the AI instead of the
entire file.  To do this, it might be beneficial to generate unique IDs for each
line number.


PLANNING AND PROJECT TRACKING WINDOW

This represents a broader usage of the LLM within an app beyond simple back and
forth chatting, where the app integrates other aspects. Persistent planning and
outline window or tab for projects, optional. Updated as project progresses
while also editable. A maintained tracking window.

A similar window for writing code is one, driven by an AI agent, that maintains
a navigable diagram (MermaidJS) of the project, where clicking on an element will
go to that section in the code.


CLIENT-SIDE COMPONENTS

Webview and code editing in two different tabs for updating the result.









