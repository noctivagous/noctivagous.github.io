Moving Past iOS

For tablet computers, iOS (iPadOS) has a major drawback: the simplest tasks are easy to achieve but anything slightly complicated has to be memorized.  The "user friendly" description only applies to the fundamentals of launching an app, zooming inside a document, and scrolling.  Additionally, a user can easily activate one of iOS's many side features on accident, and this is a poor state of UI design.  But actually, iOS is an undeveloped and prototype-level collection of user interface conventions.  Because it strives towards minimalism and commercial accessibility, it is nubby, dumbed down, and evokes feelings of interacting with playground equipment.  For controlling software, it insists on depriving the user of complementary physical controls like knobs, sliders, and buttons.

Steve Jobs just knew what mattered at the time-- to put together a tablet UI that worked for the hands and didn't require a stylus, and he brought in the multitouch technologies that had just been introduced by researchers.  What Apple developed with that was shrunken to become the iPhone then released in its original size as the iPad.  This can sell a lot of devices because anyone from the public can use what iOS provides.  But it won't ever achieve what a PC (desktop computer) can.  Making a device blunt in its simplicity comes at a steep cost, because eventually the apps that people want to make and use are more sophisticated, and software developers find that there is no systematic UI scheme available for their needs to match PC functionality.  In other words, iOS is ideologically narrow, being so determined to provide bare essentials for interactive life.  It doesn’t possess any conceptua frameworks to accommodate complex usage of a computer, so now there are PC trackpads and keyboard sold for these tablets, to turn them into PCs with screen cursors.  No cursor would be necessary if iOS carried a complete set of UI principles.

The theory of the PC was developed over a period of years by Douglas Engelbart and then Xerox PARC, so a tablet's return to the PC's conventions isn't a surprise when nothing equivalent has been developed for a multitouch device.  The theory behind iOS isn't much beyond a set of raw conveniences to accommodate the multitouch “gestures” unveiled by researchers in the 2000s, serving to launch apps and press buttons inside iOS apps.  iOS is supposedly intuitive but it's unknown to the user how to open a file inside a specific application or how to manage the filesystem because the files are no longer the focus, just launching apps, for mainstream simplicity's sake.  The objective of making everything user friendly the point of being foolproof has resulted in an uncomfortable set of devices for several years now that can scroll, zoom, activate buttons but which severely limit engagement with the computer's broad capabilities.

What iOS is really about is a false belief in a zero user-orientation period.  That is, the user should require so little time to be acquainted with iOS.  But "easy to use" (user friendly) shouldn't be equated with "no effort needed to understand it at all."  That's not a good goal in the end because of the problem it creates that were just described.  There has to be a more complex scheme set up for the future.  It shouldn't aim for being "dead simple."


Touch Modifier Buttons

An object sits there on the touchscreen, but what does touching it mean?  In real life it moves according to physics.  On a touchscreen computer, it does something so the initial answer is vague, but generally "open" or "activate".  Beyond that things get complicated.  However, if modifier buttons were placed on the left edge of the screen in a column, they would be able to specify what a touch means when it is pressed just before it is pressued.  Closing a window would not involve touching a close button but instead momentarily holding down the "close" modifier button and then touching any part of the window(s), panel(s), or object(s) that need to be closed/removed.  When a certain modifier key is held down, e.g. an "inspect" modifier, whatever object the finger touches will be affected accordingly (it will be inspected).  Touch modifier buttons will momentarily specify what a touch/gesture means.  That is to say, five objects can be removed from a page quickly by holding down this modifier, tapping each of the five objects, and then letting off the modifier key.   This will then allow the user to control the purpose of a touch in real time, which is much faster than opening up five close buttons and touching each one.  

Of course, when there are several touch modifier keys to the left of the screen, they can accommodate a wide range of purposes and this is a core feature of a new tablet device.  A “generate” modifier will generate an object at the position of a touch in an empty space or it will generate a child object for the touched object.  For example: five objects can be generated by holding down the "generate" modifier button and then tapping the desired locations of the five objects.  A few of these physical modifier buttons can be made programmable (either for controlling specific apps or the global user interface of the device).  

In this scheme, the user will always know how to accomplish tasks because the modifier keys will cover a broad range of activities, because "close" can apply to deleting, exiting, or closing something.  It is also possible that something is deleted not through the use of a "delete" modifier next to a "close" modifier but instead tapping an object twice with "close," with it darkening as a warning that it will be deleted will be the way to delete something.  Other modifier keys include "route" such as for opening a file in a specific application, because "route" is a very generic keyword that can apply to many situations.

We could mention many buttons, such as "group" touch modifier button.  But often operations are in opposition to each other. Opposing touch modifiers (group --- ungroup,  add --- subtract,  generate --- remove) on opposite sides.


NOCTIVAGOUS.GITHUB.IO