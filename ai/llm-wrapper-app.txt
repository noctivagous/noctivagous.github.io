MAKING THE LLM GO MAINSTREAM

A Versatile, Fully-Developed, General Use
Software Application Suite 
Acting As An LLM Wrapper Is How
Current AI Technology Can Finally 
Be Adopted By The Public


noctivagous.github.io


The potential and utility of an LLM is still hidden.
A much more developed category of wrapper app for an LLM
should be the priority of the AI community, one that goes
beyond the basic AI chat format in which the user is
given a free-form textbox and responses are stacked vertically.  
Interacting with the LLM is confined to typing out prompts from scratch
each time; there is no developed workflow for modifying
prompts, crafting them parametrically, or viewing the 
output in different views or presentations.  There are 
many uses for an LLM but the user is supposed to reduce 
them into a text box. Whatever comes out is supposed to 
suffice if it is just a stream of text, a chat response.
Few genuinely sophisticated features have 
been provided on the front-end to serve the end user, 
at least by the major AI vendors.

The new user is placed in front of the text box and then
told, "Now go! Ask for whatever you want!"
which sounds great at first, like an opportunity to interact with a genie
in a bottle.  In reality, instructions for work jobs are more complex
than discrete and simple requests made out of sentences. They have
parameters, settings, modes that surround the request the person
is making.  In addition to this, the person should be surrounded
by available resources such as the images, documents, and snippets
of text he wants to make use of with this machine.

Current AI technology cannot succeed by way of simplistic chat-formatted responses
generated from an LLM.  After all, in the workplace a person is provided
a complete document of specifications to carry out a job, not just a few sentences. 
The analogue to this for an LLM would be a set of parametric components, 
settings, controls, and areas for stored resources that can be included in
the queries.  The queries themselves need to be made in different ways,
whether it is typing them out or wiring them up.

The "genie in a bottle" format in which the person gets a wish
granted with a sentence, its response format in which the person
clarifies or corrects the response provided is limited and
contrasts with the natural condition of everyday life in which a 
set of commands comes in structured, in bulk, and may (as in the
case of specifications) have charts and other non-verbal descriptions
of the demands.

When you want something complex done do you tell a person who doesn't know you 
just one line or a few sentences?  Will it all be verbal information?
No, you send them a document with detailed specifications and it may have
a table included.  So the LLM has to be able to receive
commands in all kinds of ways and the only way to do that is to provide
a complete suite for input settings, input methods and output setting
and output methods.  What should be part of any query would be
a bundle of things and the app makes this possible.

The small LLM text box amounts to a prototypal, unfinished situation
in which the user laboriously starts from scratch each time to compose
short prompts, writing instructions out all over again for every minor tweak
when something wasn't quite right, writing paragraphs of prose and 
complete sentences for every revised interaction.  Because the LLM 
might not provide what was wanted the first time, revisions and tweaks 
are inevitable for everyone. It's all too exhausting and incompatible 
with mainstream adoption of AI.

To surpass the limitations of the AI chat format,
this future app should seek to provide an application suite
of prompt-making functionality, crafted by the product makers
in accordance with how people generally end up using (or could use)
an LLM, divided into modules or categories inside the app.  

Categories and app sections can include the following.
The first is the most freeform.  Each category, when activated
displays a collection of specialized controls and functionality
specific to that category.  For dealing with text,
common text software tools are built into that mode or section
of the app, something of value to many scholars.

1) (Default Mode) - Continue / Comment On / Make Response to (provided text).
2) Inquiry/Explain a Subject - Generate a profile or background on a topic.  
Provide reading list for subject.
3) Manufacture / Generate - literature, poem, essay, possible business names, recipes, etc.
4) Programming / Code -  Templates, Generate Code, Evaluate Code, Rewrite Code.
5) How-to (How to Do a Thing) -  Solutions for a Problem.  Practical Skills.  Everyday 
Reference. Domestic chores.
6) Veracity Check - (find out if something is true generally).
7) Work with Texts -  Explain provided (especially older) text.  Translate. Dictionary word lookup. 
Text passage origin / its meaning / summary.  Extract summary of only
certain type of information from the document.
8) Data: Generate or Retrieve (e.g. avg. weather for a region).  Reformat operations for 
provided data. Check for a condition inside provided data/text.  Analyze.
9) Converse or Simulate Interaction: role-playing, simulated interviews,
language learning, and other interactive sessions designed to mimic real-life interactions.
10) Education: Provide typical course curriculum for a subject.  Describe prerequisites for taking up a subject.
Make quizzes, tests.
11) Proofread / Copyedit / Rephrase



----------------------------

--- SETTINGS IN THE UI ---

PARAMETERS, BUTTONS, AND CONTROLS.


CURRENT OUTPUT MODE: [RTF] (HTML/PDF/TXT)


OUTPUT RESULTS OF QUERY: 
	- to a table
	- to a code text box
	- to a web page

	
STYLE THE RESULTS
	- with headings
	- body text font: [Times New Roman] (14pt)
	- heading font: [Arial] (Bold) (18pt)

EXPORT RESULTS AS
	- spreadsheet
	- csv
	- html
	- json


CONVERT
	- [json] to [xml]
	- [html] to [rtf]

------------------------

--- BINS, HOLDING AREAS ---

--- Separate, stationary holding area for documents to be used in queries
--- Separate, stationary holding area for pasted text snippets to be used in queries
--- Separate bin for images made part of queries

----


FETCH AND FORMULATE REPORT OF IMAGES, GALLERIES

It might be easy to overlook that at the current stage the
LLMs have not been set up to fetch images from the web
in response to a query on a certain topic and place them
in context of the query.  Let's say that a person wants to 
research edible mushroom plants. There will be many websites 
that provide information about them but this will take a lot
of time to go through them to get a complete overview, perhaps.  
There would be value in having the AI compile 
a report about the topic that is complete with images,
captions, and so on, from multiple websites, not for the
purpose of re-publication but for providing the user
an overview on a less known topic that might be somewhat unpleasant
to research on the Internet.  "What are all of the different
types of edible mushrooms grown for cooking meals?" would
return a list of images and captions.  Optional would be
a summary next to each mushroom type.  This is beyond what
Google provides at the moment.

AN IMAGE REMINDING THE USER OF THE VAST AMOUNT OF INFORMATION
CONTAINED IN AN LLM AND THE MANY CAPABILITIES.

One of the issues of leaving interaction with an LLM at
a simplistic chat text box is that the user is not in
touch with what the LLM provides.  Within the AI community
there are many criticisms of the LLM but no matter what
these could be truly useful bases of knowledge and profound
tools if they are provided the right interface and the
right indications that they are massive, they are thorough,
and they are capable.  One does not get this impression
when clicking inside the text box and then getting a response.
In this way too much is left on the user to figure out.




  
  

An LLM is so versatile but the barrier to its mainstream
adoption is the fatigue produced by typing out prompts each time,
the lack of organization, structure and guidance provided by such a simplistic user interface,
and the lack of output formatting. So, the AI community should produce common tools 
that address this, providing a kind of notebook (sections of generic use 
categories), allowing the user the opportunity to select how it should be 
used when approaching it.The user approaches the LLM with a text box today 
but will instead walk up to adeveloped suite of options. The goal 
should be to allow the user to produce the prompts rapidly by providing him or her
with parameter controls.  Each category provides its own 
premade parameters and settings, setting up a kind of mode
for interacting with an LLM.  Mode #7, text mode supplies software 
features and workflows will be specific to that category.
An application suite is what will allow AI in the form of an LLM to 
be of value.Put differently, we who use them are just those who are 
willing to put up with such a basic situation because we are accustomed 
to rough edges from technical problems.

The interface for an LLM should be multifaceted, not limited to what the
user can come up with inside a text box. A text box is a prompt for the user to
come up with something impromptu. But our app is a 
presentation of pathways for any user who opens it up.  
Each pathway, each destination has its own utilities built in.  
For this reason, it is conceivable that a person would spend all
day in our app interacting with an LLM.

When the LLM is made use of for reading old texts, for example, it has traditional
software functionality provided for that purpose, for paginating what is pasted, 
for treating a PDF as a book that has pages that can be flipped or
laid out as an imposition (overview) sheet, for highlighting text, etc. This is 
in contrast to the AI chat that has little recognition when any common, 
specific usage occurs apart from writing code, which is when it will 
provide a code box.

Allowing the output of an LLM to break out of a vertical chat format
is important as at times there may be value in generating 
responses on a grid flowing from left to right, to see iterations.  
It just has to be emphasized that if an LLM can generate all kinds of
data, then the means to present that data and generate it should be varied.
If the LLM is used to generate specific kinds of data, the presentation of 
the generated output should adapt, just like described, 
so that it isn't always raw text stacked vertically. If it generates
chart data, the data should show up in a chart. If it is a quiz being
generated, then ideally an actual set of quiz controls should show up
in the response.  A recipe should be formatted as a recipe card if possible.  
Shaping the LLM's output presentation will make these very expensive
AI models valuable. When AI is viewed as a more advanced tool rather than a 
simplistic genie in a bottle paradigm in which a question is asked an an action follows, 
these goals will be discussed more often.  Tech often gets ahead
of itself, wants to be more than it actually is and obsolesce the present
to feel like it is moving into the future.  But the basics of
traditional software remain.



NOISE IN DATA IS COMPRISED OF...	
	
-----

1) Continue/Comment On/Make Response to: This would enable users to
build upon or have the LLM respond to existing text, whether it's their own or
provided by the LLM.  There are  dialectical benefits for
the user of the LLM in this module. There is the ability to see
a broader view of the topic by having the LLM response.

2) Inquiry/Explain a Subject: Users could utilize the LLM
to research and generate informative profiles or backgrounds
on a wide range of topics, as well as receive curated reading
lists to further explore those subjects.

3) Manufacture/Generate Text: This would allow users
to generate various types of written content,
from literature and poetry to business name ideas and essays,
based on specified parameters or prompts.
The LLM could draw upon its broad knowledge to produce
 high-quality, original text tailored to the user's needs.
 
4) Programming Code: This section would enable
users to leverage the LLM's capabilities for
software development, including generating code,
answering programming-related questions, and
even performing various code operations like
refactoring, optimization, or debugging.

5) How-to and Solutions for Problems: This feature would allow users to
obtain step-by-step instructions or solutions for a variety of
tasks and challenges, drawing on the LLM's extensive knowledge
and problem-solving abilities.  Especially relevant to household
tasks, home improvement, automotive repair, questions about
machines, appliances, etc.

6) Veracity Check: Users could use this feature to assess the
truthfulness or accuracy of claims, information, or statements,
leveraging the LLM's understanding of factual knowledge and its
ability to cross-reference sources.  Primarily valued for
circumstances relating to factuality outside of contemporary
controversies.

7) Explain Provided Text: Users could leverage this feature to gain
insights into the origin, meaning, and context of a given text,
helping to deepen their understanding.  Especially useful for
older texts.  Translation, summarizing, etc. for provided texts
falls under this category.

8) Data Generation and Analysis: The LLM could be used to generate,
format, and analyze various types of data, from weather forecasts
to statistical insights, providing users with valuable information
and insights from data.

9) Converse, Simulate Interaction: This section would allow users
to engage in more immersive and interactive experiences, such
as role-playing, simulated interviews, and language learning
exercises, helping to bridge the gap between the LLM and real-world interactions.

10) Education: The LLM could be utilized to provide information on typical course
curricula for different subjects, as well as describe the prerequisites and
other relevant details to support educational and learning goals.
Test questions and quizzes can be generated in this section.

11) Proofread / Copy edit / Rephrase For Editing


Consider that conventional web search engines are 
limited in the same way, that often there is an inclination on the part of the
user to make web searches carrying more specific conditions 
but there are no settings on the search page that 
are provided for this for fear that it would make using 
the web search engine too technical.  But when a web search 
is made, sometimes there is a desire to 
limit the results to entities in the physical world, to
make sure that results exclude Internet entities.
Often there is a desire to limit a list of these results further 
to a geographic boundary, too.  Web search could be said to be in 
the same state as the LLM chat format, something simplistic. 
Just like an LLM, there are possible categories of usage that 
won't make it difficult for a lay user 
to use the technology. But because of an online culture 
that seeks to make everything minimalist, the potential of many
technologies is obscured.  Minimalist is pursued as an ideal,
to the detriment of usability and completion of the software needs.

Even when you instruct humans you give them long spec sheets 
and documents with bullet lists. "Genie in a bottle" commands, 
making consecutive small requests to the AI, isn't broad enough for
the AI to be of value for everyday tasks.  The AI tool
has to be able to take in a blueprint all at once and
the parameters of this have to be easy to change.

---

BUILDING LLM PROMPTS

-- Rectangular building blocks components replace typewritten
prompts.  Each component contains GUI controls for its parameters
and is around 200pt x 200pt.  When there is more than one they 
flow like text.  A prompt is made out
of one or more of these building blocks.
Each building block makes up the prompt, allowing rapid
construction of parametric prompts rather than just verbal ones.
The container for making a prompt out of these components 
can be made as large as a page.
-- The AI itself can be requested to make relevant prompts
and this will be incorporated into the app.

STANDARD APP SOFTWARE FUNCTIONALITY

- Export contents of selected output control
to document, .RTF, .PDF, .TXT, .CSV, .XLSX.
- AI-generated tabular data appears as interactive table
in the app.
- AI-generated map data appears on an interactive map.
- Print selected output control.
- Functions for reformatting data output are provided.


PROMPT REVISION	FUNCTIONALITY
- Qualities: add [more or less] of [quality].

CHAT OUTPUT TAIL MENU
- A menu appears at the bottom of every chat response
allowing the user to modify the original prompt 
or respond to the chat.


---

In an app like this, menus divide interactivity with the LLM into
categories that assist the user in generating prompts according
to the needs when the LLM is used.  Providing starting points for the LLM
for the user will end up being far more productive than a bare text box, which
is probably too simplistic and tiresome when there is
such a wide range of situations for which a person might use an LLM
and such a large number of instances when the prompts could be
formulated with convenient, traditional app user interface.
At the current time, this is not regarded as a concern as there is
belief that AI technology is advancing so rapidly that the tool
just described will be obsolete as soon as it is finished in a year or
two.  That may not be true.


A weak point of the AI chatbot is that interacting with the LLM
often involves a lot of repetition and revision of writing and typing
but there are no functions provided for that or even something to
formulate prompts generatively in the first place.  
The presets within each category will facilitate valuable
usage of the LLM for that individual category.

An LLM wrapper app like this demonstrates product development
that extends beyond the basic AI chatbot because it works with
the LLM with more developed interactive conveniences and it shows
that product is not obsolesced by research and engineering but
should continue to be discussed.

Currently the textbox for an AI chatbot is freeform typing.
Offering some organization and structure to the situation--
laying down a kind of blueprint for the amusement park--
will make the LLM a lot more useful for the consumer.
It is only under these conditions when an LLM could
become mainstream.



