
Moving Beyond Apple's iOS to the Future — (noctivagous)


noctivagous.github.io


The iOS UI Is Reductive; It Limits Tablet Computers

For tablet computers, iOS (iPadOS) carries a major drawback: the simplest tasks are easy to achieve, but anything slightly complicated has to be specifically learned because it is invisible, offscreen, or buried in conventions.   The "user friendly experience" only describes the basics of launching an app, zooming inside a document, and scrolling.  After that, the system software interface features are tacked on, one after another.  The consequence is that a user can easily activate one of iOS's side features on accident, and this is a poor state of UI design.  But also, iOS is an undeveloped and prototype-level collection of user interface conventions in the first place, incomplete except for consumption of media.  Because it strives towards minimalism and commercial accessibility, it is nubby and dumbed down, limited for professional usage.  For the user's control of the tablet software, Apple, characteristically, insists on withholding from the user any complementary physical controls such as knobs, sliders, and buttons, believing that those were the problem in the first place.

If there are so many problems with the tablet UI, where does Apple's success come from?  Steve Jobs just knew what mattered at the time, to put together a tablet UI that worked for the hands and didn't require a stylus to navigate the interface. To accomplish that, he brought in the multitouch technologies that had been recently upgraded by researchers (see 2006 video: https://www.youtube.com/watch?v=ac0E6deG4AU).  This technology had been available for a few decades but had been unutilized for consumer products.   The minimalist design ideology Jobs brought to the device was a secondary aspect, and it isn't the actual foundation of the iPad's success.  Rather, iOS is just a minimalist's implementation of multitouch screen and "gestures" on a tablet, artificially restricting it to sealed off "apps," which is a self-serving format for tablet software.

In chronological order, Apple developed first a large, tablet-sized multitouch screen and then shrank that to make the iPhone. Then a few years later Apple released a multitouch interface device in its original, larger size as the iPad.  It is of course very popular and has also become a definitive reference for other companies that no one bothers to re-examine, unfortunately.  Embedding a foolproof, kiosk type of user interface inside a mobile tablet can produce a lot of sales because anyone from the public can use a kiosk and thus anyone can use what iOS provides.  But it won't ever reach the depth of a PC's (a desktop computer's) capabilities.  That is the current problem, that no company has manufactured a truly finished touchscreen tablet UI, as Android is hardly more than a clone of Apple's products.  Application software will never be programmed on the iPad itself without the use of desktop computer accessories that make it behave like a PC.  Currently Apple's software development system (Xcode) is not available for iPads.  The stock UI of an iPad cannot accommodate it.

It's the design ideology behind the user interface that has caused this. Making a device blunt in its simplicity comes at a steep cost because eventually the apps that people want to make and use become complex, and software developers lack a systematic tablet UI scheme fulfilling their needs that could mirror desktop computer functionality.  When iOS is ideologically narrow, deeply determined to package bare interactive essentials as the environment for mobile computing life, it doesn’t seek to provide any conceptual frameworks to accommodate complex usage of a tablet computer.  So necessarily there are PC trackpads and keyboard sold for these tablets, to turn them into PCs with screen cursors.   No cursor would be necessary if iOS carried a complete set of UI principles, but achieving that will involve breaking out of Apple's own ideology that shuns accessory physical controls on the mobile device.  Whenever the iPad software is intended for serious usage, a trackpad basically has to be utilized.  Sony, by contrast, did not take this viewpoint about external controls and it is Apple's that has spread through society instead, influencing the entire computing industry.

The theory of the personal computer was developed over a period of years by Douglas Engelbart, and later Xerox PARC, so a tablet's return to the desktop computer's conventions is inevitable when nothing comparable in terms of depth has been developed for a multitouch device.  Apple and even Steve Jobs did not know this, as they were sure that the iPad could result in the "post-PC era" as soon as it was launched, indicating that Apple has always been terribly superficial when it comes to theories that underly the computer's provided experience.  The theory behind iOS isn't much beyond a set of raw conveniences to accommodate the multitouch “gestures” that had been unveiled by researchers in the 2000s, serving to launch apps and press buttons inside iOS apps.  Apple's specific packaging of that effectively halted the industry's future development of touchscreen UI because manufacturers were only willing to copy Apple and Apple itself was holding to its own ideology that doesn't go past a certain point.  For example, iOS is supposedly intuitive but it's difficult for the user to figure out how to open a file inside a specific application or how to manage the filesystem in a sophisticated way because the files are no longer the focus, just launching kiosk-like apps, for commercialized simplicity's sake.  The objective of making everything user friendly the point of being foolproof has resulted in an uncomfortable set of devices for several years now that can scroll, zoom, activate onscreen buttons but which severely limit engagement with the mobile computer's wide-ranging capabilities.

The evolution of the touchscreen tablet UI can begin by discussing what kinds of controls can be added to the device and what purpose they will serve, which will not end up being what is typically expected, to manipulate numeric parameters in apps.  For example, in a music studio there are many sliders on the control deck (the mixing console), they correspond to audio settings of the song like volume, and they adjust numeric values.  Affixing this type of physical control to a tablet has to be done differently, incorporating a broader conceptual framework, because software is conceptual and manipulating numeric parameters is not.  Thus, when the physical controls are added to a tablet they have to be considered in many systematic terms relating to software.  The result is not supposed to end up like what a person finds on a home stereo or electronic music equipment.  What is wanted is for the physical controls is that they help the user work with the software on touchscreen and thisis not just a matter of adjusting the software's properties.  We start by talking about touch operation buttons, a set of LCD screen-backed buttons placed to the left of the screen.



Touch Operation Buttons

An object sits there on the tablet touchscreen, but what does touching it do?  In real life it would respond according to physics.  On a touchscreen computer the object is interactive, as if alive somewhat, and currently the initial answer is vague, but generally "open" or "activate".  This is an operation, of course, and that is what differentiates the real world physics from the preset conventions of an interactive touch event on a tablet device— that the touch performs a computer operation on a screen element.  As mentioned, on iOS, an individual touch is basically defined as one operation: "activate".  To make the touch do anything else, beyond that function, requires learning all of the complicated steps afterwards, such as pressing down for longer to get something to appear.  The touch could be made to perform lots of different functions (other operations on the screen elements), but right now it can only perform "activate".  For a new type of tablet device, the touch event needs to be expanded to other operations as well.

If about ten physical buttons were arranged in a column just to the left of the screen, they could specify what a touch means when it occurs, and this will broaden the tablet's user interface a great deal and make the UI far more nimble.  Closing a window would not involve touching a close "x" that appears on a window object but instead holding down the "Close" touch operation button on the left edge of the screen and then touching any part of window(s), tab view(s), panel(s), or object(s) that need to be closed, then releasing the "Close" button.   When a certain touch operation button is held down, e.g. an "Inspect" button, whatever object the finger touches will be acted upon accordingly (it will be inspected, it will be copied to the clipboard, etc.).  More than one object can receive that operation while the button is held down, making for a quick and capable UI.  So, closing a tab and its contents does not involve touching the small close button on the tab but instead holding down "Close" and touching any part of the tab or the viewframe (such as a web page) associated with the tab.   The user then does not need to be concerned with hit target accuracy.

To summarize: a touch operation button momentarily specifies what a touch means just before the touch occurs and then the physical button is released by the user after the touch is finished.  The speed and straightforwardness of the interface improves: five objects can be removed from a page quickly by holding down a "Remove" operation button, then tapping each of the five objects that need to be removed, then releasing the "Remove" touch operation button (that sits to the left of the screen in the column).   This setup will allow the user to control software with a wide range of touch operations instead of searching for operations deep inside the interface through menus or convoluted maneuvers.  In addition, the column of physical touch operation buttons can have groups of  operations swapped out because the buttons  each carry their own LCD screen, which is now a feature of mainstream consumer devices related to live video streaming.

Thus what is convoluted to achieve today on a tablet will not be, and what is out of reach for the average user who does not bother to memorize specific features will become straightforward for anyone when these physical buttons are placed onto the tablet computer.  The key benefit when there are several touch operation buttons sitting to the left of the screen is that they can accommodate a wide range of touch objectives as soon as the tablet is picked up, and this is the core of a new tablet device framework for user interaction.  A “Generate” touch operation button will make the touch generate an object at the position of contact in an empty space (or it will generate a child object for the object that was touched).  To illustrate: five objects can be generated by holding down the "generate" touch operation button and then tapping the desired locations of the five objects.   For power users, some of these touch operation buttons can designated for custom-programmed operations and scripts.  Each touch operation button has an LCD screen, which is why they are so configurable.

Importantly, in this scheme the user will always know how to accomplish tasks because the touch operation buttons will be general in their function names, covering a broad range of activities, and so they will naturally work in combination with each other. "Close" can apply to exiting or closing anything, usable for many types of objects on screen, not just windows.  "Inspect" can apply to a large number of objects as well.   Just these two example buttons already work in combination: any inspection popover opened with "Inspect" can be closed with the touch operation button "Close", demonstrating that the operations work systematically already just by being general.  "Open" can begin the playing of the touched song while "Close" can pause that song. A user can know how to do all sorts of tasks without specific instructions when the provided operation buttons are helpful.  The current procedure for the user in iOS is to extract these functions (e.g. making close buttons appear) and this replaces that unpleasant dynamic.

More touch operation buttons:

-- "Teleport" - This replaces dragging on the touchscreen.  The first touch establishes which object is to be transported to another part of the screen (what is to be dragged) and the second touch moves it to its desired location immediately, thus the name teleportation.  So there are only two touches to accomplish what dragging does today because "Teleport" is held down.  This is much faster than dragging objects on a touchscreen and will feel much more precise.  To recap, the "Teleport" touch operation button is held down during the first and second touch, then it is released.  This opens the door to other types of related operations: if it were "Copy" that were held down, the first touch would establish what is to be copied and the other touches afterwards would be copy destinations, with there just being one if there is only one copy destination.  Then after all of the touches, "Copy" is released.

-- "Select" - While "Select" is held down, the finger can select text as it moves along the surface of the screen, or it can tap multiple objects.  Compare the use of this touch operation with the current, ambiguous situation in which the iPad's selection mode for text has to be triggered through timing (holding down the touch for a few seconds), which is confusing and can also be activated on accident.  Touch operations clarify what is happening.  Also, the value of touch operations is that when "Lasso" is available as a touch operation button and it is held down, there is no need to take over the entire app with a tool mode such as how a lasso tool in photo editing programs is used.  Instead "Lasso" is held down, the finger draws the selection outline and then "Lasso" is lifted.  In the same way, a circle is drawn not by activating a circle tool but by pressing down the "Circle" touch operation button and drawing the diameter with the finger then releasing the touch operation button.

-- "Toggle" - This will turn a touched setting or object on and off, opposite its current state and this can apply to many circumstances.

-- "Replace" - The object to be replaced is tapped followed by the one that will replace it (works for graphical objects, text, and also fill colors).  This pairs with "Swap" in which the two objects' positions are swapped after tapping both of them.   With "Apply", the application of a style to a recipient object, such as a color setting applied to an article of clothing or font and color combination to a body of text, can be achieved easily by this touch operation button.  The style itself is loaded into "Apply" with the companion "Extract" touch operation button.  In all of these cases, whether it is "Replace",Swap", "Apply", "Extract" there is no predefined type of object that has to be the recipient of the operation.  With the style-related operations, there is no predefined type of property that they work on.   This demonstrates how touch operation buttons have wide applicability when they are given generic titles.


Fortunately, the touch operation buttons can reduce the need for what are currently called "gestures" because even a replacement "Zoom" touch operation button will provide more precise control over the zoom while "Zoom" is being held down, allowing the user to go even slower than today.  Likewise, a "Rotate" touch operation button will provide greater real-time precision for the user than a rotate "gesture". In the case it is still desired to have "gestures," there can be many types of swipe, rotate, or two-finger scrolling according to the touch operation buttons.  But since zoom, rotate, and pan are such commonly accessed features they may have their own dedicated touch operation buttons on the device anyway.   If they do not, the remaining advantage of accepting "gestures" is that they allow freeing up button space in the touch operations column.

Touch operation buttons should serve to replace the current user interface conventions on tablets, which is timing-activated functionality (holding down for a few seconds) and digging into menus to find the desired function.  If these touch operation buttons are categorized, color-coded, and grouped adequately, an increase in the number of operations will not overwhelm the user but instead make it so the user does not need to be provided tutorials for most basic tasks.  On the device, touch operation buttons can be assigned background colors on the LCD-backed buttons according to category, allowing the user to know what a touch operation button is for.  How to swap out groups of touch operation buttons on the column of buttons becomes a question.  Forward and backward arrows are probably not sufficient.  Instead, the individual categories of touch operations can exist as separate buttons in a grid just beneath the column.  This way, touch operation buttons can be the center of interacting with an app's functionality.

The goal of the tablet's design is that app functionality is activated primarily from outside the touchscreen itself, such as through the use of touch operation buttons, instead of having the user learn a wide variety of conventions specific to each app that are all inside the touchscreen.  Whether it is editing of text or rotating and scaling objects, many apps share functionality and they have common needs, so a tablet should be able to offer access to nearly all of those by way of physical controls placed on the surrounding the touchscreen screen, with the touchscreen just acting to communicate to the app where the user wants the touch point at any given time, as it is mediated by the surrounding controls.  Many types of apps, for example, need to constrain movement, whether it is on the X or Y axis or along a specific angle.   What has to be considered is how the controls are placed onto the device with a deeper, systematic plan, that they are not just for manipulating individual parameters (like a slider or rotary knob) as found on a kitchen appliance or home stereo, which is what controls have always been for.  Instead they embody concepts, because this is a computer that has to accommodate software with its controls and it is not a washing machine that has a static group of settings.  When the physical controls represent concepts related to apps and software, the software will not rely solely on the touchscreen to control the app, which presents problems.

To unlock and lock the device, there can be a "Password" touch operation button that, when held down, accepts a simple finger pattern on any part of the screen, at any scale of the original pattern, for either locking and unlocking the device (whether it is a very simple set of "x" strokes or something else).  This shows that there are some benefits in reducing the number of touches on the device that are not first clarified by a touch operation button held down at the same time.

It's also worth noting that even the two-finger scroll gesture can be replaced for vertical scrolling with a single touch operation button.  The touch operation button is held down with the left hand and the two fingers on the right hand tap the screen, with each individually indicating whether it is to scroll up (index finger) or down (middle finger).  Light taps scrolling in a burst up or down  whereas holding down one of the two fingers on the screen resulting in continuous scrolling.  Typical scrolling would then commonly be characterized by a rapid sequence of finger taps.

Touch operation buttons also can serve to activate a horiziontal or vertical slider that appears on the touchscreen or, more preferably, be the means of assigning what a primary rotary knob on the device does while the touch operation button is held down.  The goal is to reduce the amount of interaction with arrays of controls, such as sliders, on the touchscreen itself, to provide a situation that is superior to reproducing mechanical controls on a touchscreen (or even as they exist on the original machines).




Opposing Touch Operation Buttons

In this column of physical touch operation buttons on the left side of the scren, it might seem best at first to have just one column.   But often operations exist in opposition to each other (group --- ungroup,  add --- subtract,  generate --- remove, open --- close, show --- hide).  Two columns of opposing operations would likely be preferable, doubling the number of available buttons that are sitting to the left of the screen.

The touch operation pairs are general purpose.  A window is closed with "Close," but "Close" can also be used to turn a switch control to the off position and close just about anything else.  "Open" opens anything in the same way and turns something on.  
"Expand  --- Collapse" is about unfolding and stowing away and applies to windows but also collapsible controls.  That pair of operations addresses a different state dynamic than "Open --- Close". Pairs of buttons would have generic names which apply to many scenarios.




The "Multiple" Touch Operation Modifier

Like the shift key on the keyboard, the "Multiple" key on the device allows selecting more than one object at a time.  It is a modifier button, which is what the shift key is.  During typing would act as the shift key, too.  Where this key is placed is not certain currently, but could be on the side, where less frequently accessed keys are normally accessed.




Mode Buttons Column

On the right side of the device's screen can sit mode buttons that are utilized by all apps.  These buttons do not modify the touch but instead the entirety of the app's mode or the mode of a certain context.  All apps running on the tablet would utilize these buttons as a primary interface, because all complex apps have states or modes, a mode generally regarded as a large collection of states and functionality like on any consumer machine.  Modes are not generally discussed as a topic specifically in software development but they are a central aspect, as changing a tool in a photo editor is changing a mode and the toolbar is a collection of modes.  It is just that the names "toolbar" and "tools" were used instead.   Most apps carry a variety of modes, with viewing and editing modes being the most fundamental for apps that produce data of some sort.  When apps can be written around a set of physical mode and state controls, the apps will be higher quality for user interaction. The conceptual internals of an app and computing system are being represented on the outside of this device, next to the screen.

Again, each of these buttons will be backed by individual LCD screens to give the software developer's flexibility.  The purpose of these mode buttons is to support anything that is a mode or state.   Configuring preferences is another mode, and when both the software developer and the user thinks in terms of "mode," because the device is built that way, there is less need for the user to look up specific conventions, such as for opening the settings window.  (It's also possible to make a "Configure" touch operation button so that with a touch anything on the screen can be configured, but this would not be a mode but more like a way to trigger a configuration popover).

Other uses of modes include switching on and off submodes of editing in the app.  The mode buttons should be able to accommodate anything needed involving modes, submodes, and states.




Thumb Input for Text Words and Characters: Mode Buttons and Touch Operation Buttons Simultaneously Enter a Meta Mode, to Replace The Conventional Onscreen Keyboard, Providing an Input Dynamic Through Alternating Thumb Input from Each Side

When typing is needed, the LCD-backed buttons on the device enter a meta mode for this purpose.  At the same time, the mode buttons on the right and the touch operation buttons on the left transform into input buttons for a specific word and typing entry system, with each of the two sides able to receive input back and forth in alternation by the thumbs, for the user to enter words and characters.  The two sets of buttons have entered a meta mode for input, repurposed for entering words. What shows up on the LCD screens for each button on both sides can go beyond individual characters of a keyboard, to allow entering words without autocomplete. The entry will be both tactile and accurate because the buttons themselves are physical and it can be the alternation of pressing buttons on both sides that completes words.  The buttons can display anything, including full words and even symbols or images, so meta modes are an expansive area in the case there are other needs for all of the buttons.  What is static on the device are reaction buttons such as "Escape/Cancel" in the upper lefthand corner" and "Accept/Proceed" in the lower right corner.

What is pressed on the buttons can affect what shows up on the left and right sides of the screen vertically, lining up with the buttons themselves, so that the user of the device can see the effects of pressing buttons in alternation on the screen.

A meta mode is available tool for any app for when complex entry of data would benefit from taking over the device's buttons and putting them into a special mode, but it would be utilized less commonly than what is provided.  At all times the "Escape/Cancel" button on the device would exit such a custom meta mode.

Lastly, it is possible to supplement this data entry meta mode through buttons placed on the underside of the device (as currently found on handheld video game consoles), and it is possible this would be an important part of text entry.




Reaction Buttons Such as "Undo" and "Redo"

One could say that pressing "Undo" is unlike other types of functions activated in that it is a reaction to a mistake, thus this is a reaction type of function or button.   When something relates to reacting, it is a candidate for a physical button instead of space on the touchscreen.  In an app, "Undo" and "Redo" act on a maintained list of user operation history that is most often out of view.  The two "Undo" and "Redo" buttons will not exist as touch operations because they act on the last operation done.  They sit next to the touch operation buttons because they undo or redo what was just done by a touch operation.

Reaction buttons can be said to be those which are used by the user quickly in response to an immediate change or request from the app. The periphery of the device is a suitable place to place these reaction buttons because the hands are close.  For example, if anything isn't right or should be exited, the user will want to press the "Escape" button in the top left of the device and it takes no time to press it.  If something needs to be confirmed, the user can press the "Accept/Proceed" key in the bottom right corner of the device.  Why include physical buttons when a person can touch the screen itself?  Because the less movement of the hands the better with respect to these types of user actions.



Union (Add), Subtract, Intersection Buttons



No Confirmation Dialog Boxes ("Are You Sure?") Needed Because of Implicit Confirmation Pathways

If an object is selected and it is of sensitive value for deletion (deleting it could create a problem if done on accident), then the following is how to delete it: the "Remove" touch operation button is held down and that object is tapped three times, with the object darkening with each tap.  On the third tap it is deleted. Thus, in this case there is no need to display a warning dialog box for the user that asks, "Are you sure you want to delete this?"  The pathway for the actionhas been set up so that confirmation is implicit and takes place through the action.  Many types of confirmation dialog boxes can be eliminated this way, such as through confirmation sequences where two objects are tapped in alternation or in a pattern.




Scrolling-Averse System Conventions

Noctivagous takes the position that a large amount of vertical scrolling is harmful to the user, is uncomfortable, and is a substitute for what should actually exist more often, which is proper transitioning of content (sliding, for example) that is at all times correctly sized to the device's frame dimensions.  That is, content should generally be swapped in and out carousel-style or otherwise, faded in smoothly.  It can be paginated more often and the device itself would carry two buttons for forward and backwards.  Content should be clipped less and therefore require scrolling les.  Information in the future can be prepared and laid out such that it is fitted for the dimensions of the device. "Forward" and  "Backwards" slides or fades in content within the same frame, just like any ebook reader for example, and nothing has to be moved into the frame of the device through scrolling.  This is especially applicable to consumption of information.  The web itself should be refitted in this way for long pages.




Separate X and Y Axis Touchpad Strips

A hardware feature that is considered, but is optional, for a Noctivagous tablet is a pair of X (horizontal) and Y (vertical) axis touchpad strips with pressure sensitivity.  They would sit above and to the left of the screen respectively and match the lengths of the width and height.  The Y axis (vertical) touchpad strip will sit on the left edge of the screen, sitting between the Touch Operation buttons and the screen.  These strips are available for all apps and the benefit is that they constrain movement.  Apps will be able to use these touchpad strips for horizontally and vertically constrained needs.  For irregular movements the screen itself can be used.  It is natural to select, for example, an app icon by lining up the x and y axes with both fingers as they are sliding along the touchpad and then pressing down on the two touchpad strips at the same time.




Future Programming Through Touch Operation Buttons

In many ways, programming involves the insertion of containers of operations, so there is no difficulty changing the purpose of the touch operation buttons to insert various types of operations in a document of code.  In addition, with some touch operation buttons meant for code, the arrows drawn by a touch can represent various types of functionality for the program.




NOCTIVAGOUS.GITHUB.IO





----




---

With touch operations, it is also possible that something is deleted not through the use of a "delete" modifier next to a "close" modifier but instead tapping an object twice with "close," with it darkening as a warning that it will be deleted will be the way to delete something.  Other modifier keys include "route" such as for opening a file in a specific application, because "route" is a very generic keyword that can apply to many situations.


Instead of "cut [out]","copy [from]","paste [on/into]" more like "extract","


  


NOCTIVAGOUS.GITHUB.IO